# deep_nlp_project
This repo contains our project and it's description

Мы решали задачу  *Finetune RU GPT - Language modeling для другого домена*

# Данные: 
мы решили выбрать в качестве данных для обучения фанфики (любительские художественные тексты, выложенные в свободном доступе на сайте ficbook.net). Для сбора данных был написал краулер (его можно найти в отельном прилинкованном репозитории, а также можно найти непосредственно сам датасет по ссылке). Мы выбрали эти тексты, так как у них достаточно однородная структура и стилистика (есть определенные языковые паттерны и тематики, принятые в этом сообществе), поэтому нам показалось, что это поможет и проще оценить качество работы модели (стиль фанфиков легко узнаваем), но также поможет и модели легче настроиться на этот домейн. В результате проведенного эксперимента это подствердилось. 

# Метрики: 
Perplexity

# Бейзлайн: 
здесь сложно придумать какую-то бейзлайновую модель, потому что суть задачи донастроить уже имеющуюся модель под нужный домен. Наверное, можно рассмотреть в качестве бейзлайна просто GPT3 as is на корпусе (без настройки каких-либо гиперпараметров, добавления conditioning и пр.), однако уже такое решение дает очень хорошие результаты и перплексию 10.


# Ход работы: 
сначала нам нужно было собрать данные (уже было в наличии около 1000 текстов, они были подготовлены для проекта по статистике, в них была лишняя метаинформация, поэтому сначала мы очистили эти тексты, потом обучили на них GPT-3, однако метрики качества были низкими и мы решили попробовать добрать еще текстов, для этого нам пришлось переделать крайлер, чтобы он не собирал лишнюю информацию, а так как тексты собирались довольно долго, что мы знали из прошлого опыта работы с этим сайтом, мы решили добавить еще и бд, чтобы не потерять уже собранные данные, если вдруг что-то сломается ночью, когда краулер работет). За ночь нам удалось собрать еще около 8000 текстов и мы решили попробовать обучить уже на них, неожиданно для нас перплексия была окло 10. 

# Трудности: 
Выше уже описывались трудности со сбором данных, также мы довольно долго не могли запустить саму GPT, изначально мы арендовали машину на Vast.ai, но там мы долго не могли запустить модель на GPU, не могли установить пакеты apex, были проблемы с совместимостью версий CUDA и torch, ничего не заводилось, не помогала смепна машины и в итоге нам пришлось уйти на colab. Изначально мы не хотели туда идти, так как хотели использовать модель medium, и она падала по памяти CUDA, но так как не получилось запустить на vast.ai, то нам пришлось взять модель small, и когда мы увидили перплексию около 80 на 1000 текстов, то решили, что все-таки нужно больше данных. 

На самом деле мы были очень удивлены, что смогли достись такой хорошей перплексии по сути просто запустив small GPT-3 на достаточном количестве текстов, но, кажется, что это связано с большлой однородностью корпуса. Но так как с задачей finetuning до этого мы не сталкивались, паралелько с тем, что мы собирали данные и пытались все запустить, мы гуглили, как вообще можно файнтьнить модель. Нашли библиотеку fastai и несколько тьюториалов и даже курс от них, а также статью о finetuning GPT (https://medium.com/@pierre_guillou/faster-than-training-from-scratch-fine-tuning-the-english-gpt-2-in-any-language-with-hugging-f2ec05c98787). Мы хотели сделать бейзлайн, а потом планомерно пробовать методы, описанные в этой статье от сложных к простым, но перплексия уже на бейзлайн модели была довольно низкой. 
Также прочитали статью о conditioning и залезли в их репозиторий, попытались запустить, осознали, что там есть BPE только для английского, поэтому попыталась обсучить свой BPE и токенизировать им датасет, чтобы отдать модели. А еще модель conditioning написана на tensorFlow, а GPT на pyTorch, поэтому возникли доволнительные трудности с тем, чтобы это состыковать, у них есть экпериментальная функция, которая переводит веса из одного формата в другой, но до этого надо запэтчить пакет keras 

# Кто что делал: 
Дарья Самсонова: сбор данных, ресерч
Григорий Аршинов: запуск модели 

Но на самом деле, довольно сложно как-то разделить, потому что работу мы делали вместе, и какие-то возникающие вопросы и проблемы обсуждали и решали тоже вместе.
