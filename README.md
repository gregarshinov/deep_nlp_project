# deep_nlp_project
This repo contains our project and it's description

Мы решали задачу  *Finetune RU GPT - Language modeling для другого домена*

# Данные: 
мы решили выбрать в качестве данных для обучения фанфики (любительские художественные тексты, выложенные в свободном доступе на сайте ficbook.net). Для сбора данных был написал краулер (его можно найти в отельном прилинкованном репозитории, а также можно найти непосредственно сам датасет по ссылке https://drive.google.com/file/d/1Ak2FTIQ_XYuUoTbxz_1guXlM-rG1Ep5C/view?usp=sharing). Мы выбрали эти тексты, так как у них достаточно однородная структура и стилистика (есть определенные языковые паттерны и тематики, принятые в этом сообществе), поэтому нам показалось, что это поможет и проще оценить качество работы модели (стиль фанфиков легко узнаваем), но также поможет и модели легче настроиться на этот домейн. В результате проведенного эксперимента это подствердилось. 

# Метрики: 
Perplexity

# Бейзлайн: 
За бейзлайн мы взяли стандартную ru-GPT3. Мы померили её перплексию на нашем валидационном датасете и получили значение в 30.


# Ход работы: 
сначала нам нужно было собрать данные (уже было в наличии около 1000 текстов, они были подготовлены для проекта по статистике, в них была лишняя метаинформация, поэтому сначала мы очистили эти тексты, потом обучили на них GPT-3, однако метрики качества были низкими и мы решили попробовать добрать еще текстов, для этого нам пришлось переделать крайлер, чтобы он не собирал лишнюю информацию, а так как тексты собирались довольно долго, что мы знали из прошлого опыта работы с этим сайтом, мы решили добавить еще и бд, чтобы не потерять уже собранные данные, если вдруг что-то сломается ночью, когда краулер работет). За ночь нам удалось собрать еще около 8000 текстов и мы решили попробовать обучить уже на них, неожиданно для нас перплексия была около 30. Далее, мы попробовали дообучть бейзлайн модель контролируя больше параметров. Используя transformers мы подгрузили модель сбера и токенизировали наш датасет. Далее, используя fastai обернули датасет в dataloader. После этого мы разделили слои модели на четыре группы: 3 группы по 4 attention heads и одна группа с wte и wpe слоями. Для каждой группы мы определяем свой собсвенный learning rate и используем постепенную "разморозку" по каждой из групп. При генерации текстов мы использовали 2 типа семплирования: top-k sampling и top-p nucleus sampling. 

# Трудности: 
Выше уже описывались трудности со сбором данных, также мы довольно долго не могли запустить саму GPT, изначально мы арендовали машину на Vast.ai, но там мы долго не могли запустить модель на GPU, не могли установить пакеты apex, были проблемы с совместимостью версий CUDA и torch, ничего не заводилось, не помогала смепна машины и в итоге нам пришлось уйти на colab. Изначально мы не хотели туда идти, так как хотели использовать модель medium, и она падала по памяти CUDA, но так как не получилось запустить на vast.ai, то нам пришлось взять модель small, и когда мы увидили перплексию около 80 на 1000 текстов, то решили, что все-таки нужно больше данных. 

На самом деле мы были очень удивлены, что смогли достись такой хорошей перплексии по сути просто запустив small GPT-3 на достаточном количестве текстов, но, кажется, что это связано с большлой однородностью корпуса. Но так как с задачей finetuning до этого мы не сталкивались, паралелько с тем, что мы собирали данные и пытались все запустить, мы гуглили, как вообще можно файнтьнить модель. Нашли библиотеку fastai и несколько тьюториалов и даже курс от них, а также статью о finetuning GPT (https://medium.com/@pierre_guillou/faster-than-training-from-scratch-fine-tuning-the-english-gpt-2-in-any-language-with-hugging-f2ec05c98787). Мы хотели сделать бейзлайн, а потом планомерно пробовать методы, описанные в этой статье от сложных к простым, но перплексия уже на бейзлайн модели была довольно низкой. 
Также прочитали статью о conditioning и залезли в их репозиторий, попытались запустить, осознали, что там есть BPE только для английского, поэтому попыталась обсучить свой BPE и токенизировать им датасет, чтобы отдать модели. А еще модель conditioning написана на tensorFlow, а GPT на pyTorch, поэтому возникли доволнительные трудности с тем, чтобы это состыковать, у них есть экпериментальная функция, которая переводит веса из одного формата в другой, но до этого надо запэтчить пакет keras. 
Когда мы приступили к экспериментам с более тонким контролем файнтюнига, мы столкнулись с тем, что fastai не импортируется. После некоторого времени выяснения причин, мы поняли, что эту библиотеку требовалось обновить до последней версии. Так же, мы столкнулись с тем, что каждый эксперимент занимал как минимум один час.  Для решения этой проблемы, мы арендовали машину с 4мя GPU и использовали каждую из них в отдельных ноутбуках.

# Кто что делал: 
- взяли старый датасет с ~1000 текстов, очистили от лишних тегов (они были нужны для прерыдущего проекта) (Собирали вместе)
- Взяли модель GPT3 medium запустили на colab (упала по памяти) (Гриша)
- Пытались запутсить GPT3 на vast.ai (проблемы с библиотеками и версией CUDA, так и не запустили) (Гриша)
- Пришлось взять GPT3 small и запустить на colab (запустилась, перплексия ~80)(Гриша)
- так как не могли взять модель сложнее, решили добавить данных: (Даша)
- Поправили код краулера, убрали в нем приписывание тэгов (Гриша)
- Поставили собираться тексты (собрали ~9000) (Гриша)
- Запустили GPT3 small на 9000 текстов (Гриша)
- Так как думали, что маленькой модели сильно не поможет увеличение датасета, занялись ресерчем способов файнтьюнить гпт (описано подробнее выше)
- Прочитали статью, которую вы предложили (Даша)
- Попытались запустить Conditional Transformer Language Model for Controllable Generation (Гриша)
- Нашли инструкцию по более котролируемому файнтюнигу GPT2 (Даша)
- Разделили датасет на трейн и тест (Гриша)
- Используя transformers подгрузили модель сбера и токенизировали наш датасет (Даша)
- Используя fastai обернули датасет в dataloader (Даша)
- Проверили перплексию модели сбера без дообучения на валидационном датасете, увидели 30 (Даша)
- Разделили слои модели на четыре группы: 3 группы по 4 attention heads и одна группа с wte и wpe слоями. Для каждой группы мы определяем свой собсвенный learning rate и используем постепенную "разморозку" по каждой из групп. (Даша)
- При генерации текстов мы использовали 2 типа семплирования: top-k sampling и top-p nucleus sampling (Даша)
- Анализ результатов (Вместе)

# Анализ

Нашу работу можно разделить на 5 экспериментов:
- Тренировка с использованием готового скрипта Perplexity: 30 (e^final_loss)
- Проверка бейзлайна (базовой ru-GPT3) Perplexity: 30
- Тренеровка с "замораживанием" всех слоев, кроме последнего Perplexity: 29.12
- Тренеровка с "замораживанием" всех слоев, кроме последних двух Perplexity: 29.003
- Тренеровка с "замораживанием" всех слоев, кроме последних трех Perplexity: 29.27
- Тренеровка с всех слоев Perplexity: 27.27

Судя по тренду снижения перплексии, можно говорить о том, что чем больше слоев участвует в дообучении, тем лучше модель "тюнится". К сожалению, "заморозка" не повлияла на "тюнинг" положительно. Мы предполагаем, что для улучшения результата следует дать модели обаться еще несколько эпох и сместить соотношение примеров в выборке в пользу тренеровочной, так как большее количество данных может позволить модели лучше дообучиться. Так же, можно было бы дотренеровать токенизатор и попробовать другие виды сэмплирования для валидации.

Примеры сгенерированных текстов можно найти в ноутбуках.
