# deep_nlp_project
This repo contains our project and it's description

Мы решали задачу  *Finetune RU GPT - Language modeling для другого домена*

# Данные: 
мы решили выбрать в качестве данных для обучения фанфики (любительские художественные тексты, выложенные в свободном доступе на сайте ficbook.net). Для сбора данных был написал краулер (его можно найти в отельном прилинкованном репозитории, а также можно найти непосредственно сам датасет по ссылке https://drive.google.com/file/d/1Ak2FTIQ_XYuUoTbxz_1guXlM-rG1Ep5C/view?usp=sharing). Мы выбрали эти тексты, так как у них достаточно однородная структура и стилистика (есть определенные языковые паттерны и тематики, принятые в этом сообществе), поэтому нам показалось, что это поможет и проще оценить качество работы модели (стиль фанфиков легко узнаваем), но также поможет и модели легче настроиться на этот домейн. В результате проведенного эксперимента это подствердилось. 

# Метрики: 
Perplexity

# Бейзлайн: 
За бейзлайн мы взяли стандартную ru-GPT3. Мы померили её перплексию на нашем валидационном датасете и получили значение в 30 (мы говорили о перплексии в 10 ранее, так как считали ее по формуле 2^loss, такую же формулу мы используем в проекте по НИСу, но позднее в этом проекте мы использовали библиотеку fastai и там перплексия считается как e^loss, чтобы результаты были сравнимы, мы пересчитали по этой формуле).


# Ход работы: 
сначала нам нужно было собрать данные (уже было в наличии около 1000 текстов, они были подготовлены для проекта по статистике, в них была лишняя метаинформация, поэтому сначала мы очистили эти тексты, потом обучили на них GPT-3, однако метрики качества были низкими и мы решили попробовать добрать еще текстов. За ночь нам удалось собрать еще около 8000 текстов и мы решили попробовать обучить уже на них, неожиданно для нас перплексия была около 30. Далее, мы попробовали дообучть бейзлайн модель контролируя больше параметров. Используя transformers мы подгрузили модель сбера и токенизировали наш датасет. Далее, используя fastai обернули датасет в dataloader. После этого мы разделили слои модели на четыре группы: 3 группы по 4 attention heads и одна группа с wte и wpe слоями. Для каждой группы мы определяем свой собсвенный learning rate и используем постепенную "разморозку" по каждой из групп. При генерации текстов мы использовали 2 типа семплирования: top-k sampling и top-p nucleus sampling. 

# Трудности: 
Мы довольно долго не могли запустить саму GPT, изначально мы арендовали машину на Vast.ai, но там мы долго не могли запустить модель на GPU, не могли установить пакеты apex, были проблемы с совместимостью версий CUDA и torch, ничего не заводилось, не помогала смена машины и в итоге нам пришлось уйти на colab. Изначально мы не хотели туда идти, так как хотели использовать модель medium, и она падала по памяти CUDA, но так как не получилось запустить на vast.ai, то нам пришлось взять модель small, и когда мы увидили перплексию около 80 на 1000 текстов, то решили, что все-таки нужно больше данных. 

На самом деле мы были очень удивлены, что смогли достичь такой хорошей перплексии по сути просто запустив small GPT-3 на достаточном количестве текстов, но, кажется, что это связано с большлой однородностью корпуса. Но так как с задачей finetuning до этого мы не сталкивались, паралельно с тем, что мы собирали данные и пытались все запустить, мы гуглили, как вообще можно файнтьнить модель. Нашли библиотеку fastai и несколько тьюториалов и даже курс от них, а также статью о finetuning GPT (https://medium.com/@pierre_guillou/faster-than-training-from-scratch-fine-tuning-the-english-gpt-2-in-any-language-with-hugging-f2ec05c98787). Мы хотели сделать бейзлайн, а потом планомерно пробовать методы, описанные в этой статье от сложных к простым.
Также прочитали статью о conditioning и залезли в их репозиторий, попытались запустить, осознали, что там есть BPE только для английского, поэтому попыталась обсучить свой BPE и токенизировать им датасет, чтобы отдать модели. А еще модель conditioning написана на tensorFlow, а GPT на pyTorch, поэтому возникли доволнительные трудности с тем, чтобы это состыковать, у них есть экпериментальная функция, которая переводит веса из одного формата в другой, но до этого надо запэтчить пакет keras. В итоге мы решили перейти к советам из туториала по файнтьюнингу
Когда мы приступили к экспериментам, мы столкнулись с тем, что fastai не импортируется. После некоторого времени выяснения причин, мы поняли, что эту библиотеку требовалось обновить до последней версии (так как у них вообще-то есть еще версия fastai 2, но она была вмержена в fastai) и заменить имена некоторых вызываемых атрибутов, потому что они были переименованы. После того как библиотеку удалось запустить и мы дошли до части с обучением моделей, выяснилось, что оно не помещается в 15 гб гпу коллаба и нам пришлось переходить на vast.ai. Так же, мы столкнулись с тем, что каждый эксперимент занимал как минимум один час 20 минут. Для решения  всех этих проблем, мы арендовали машину с 4мя GPU по 40ГБ и использовали каждую из них в отдельных ноутбуках (именно поэтому результаты обучения с разным количеством замороженных слоев находятся в разных ноутбуках). 

# Кто что делал: 
- взяли старый датасет с ~1000 текстов, очистили от лишних тегов (они были нужны для прерыдущего проекта) (Собирали вместе)
- Взяли модель GPT3 medium запустили на colab (упала по памяти) (Гриша)
- Пытались запутсить GPT3 на vast.ai (проблемы с библиотеками и версией CUDA, так и не запустили) (Гриша)
- Пришлось взять GPT3 small и запустить на colab (запустилась, перплексия ~80)(Гриша)
- так как не могли взять модель сложнее, решили добавить данных: (Даша)
- Поправили код краулера, убрали в нем приписывание тэгов (Гриша)
- Поставили собираться тексты (собрали ~9000) (Гриша)
- Запустили GPT3 small на 9000 текстов (Гриша)
- Так как думали, что маленькой модели сильно не поможет увеличение датасета, занялись ресерчем способов файнтьюнить гпт (описано подробнее выше)
- Прочитали статью, которую вы предложили (Даша)
- Попытались запустить Conditional Transformer Language Model for Controllable Generation (Гриша)
- Нашли инструкцию по более котролируемому файнтюнигу GPT2 (Даша)
- Разделили датасет на трейн и тест (Гриша)
- Используя transformers подгрузили модель сбера и токенизировали наш датасет (Даша)
- Используя fastai обернули датасет в dataloader (Даша)
- Проверили перплексию модели сбера без дообучения на валидационном датасете, увидели 30 (Даша)
- подобрали learning_rate с помощью lr_find (Даша)
- Разделили слои модели на четыре группы: 3 группы по 4 attention heads и одна группа с wte и wpe слоями + layerNorm. Для каждой группы мы определяем свой собсвенный learning rate и используем постепенную "разморозку" по каждой из групп. (Даша)
- При генерации текстов мы использовали 2 типа семплирования: top-k sampling и top-p nucleus sampling (Даша)
- Анализ результатов (Вместе)

# Анализ

Нашу работу можно разделить на 5 экспериментов:
- Тренировка с использованием готового скрипта (3 эпохи) Perplexity: 30 (e^final_loss)
- Проверка бейзлайна (базовой ru-GPT3) Perplexity: 30
- Тренировка с "замораживанием" всех слоев, кроме последнего Perplexity: 29.12
- Тренировка с "замораживанием" всех слоев, кроме последних двух Perplexity: 29.003
- Тренировка с "замораживанием" всех слоев, кроме последних трех Perplexity: 29.27
- Тренировка с всех слоев (2 эпохи) Perplexity: 24.8

Судя по тренду снижения перплексии, можно говорить о том, что чем больше слоев участвует в дообучении, тем лучше модель "тюнится". К сожалению, "заморозка" не повлияла на "тюнинг" положительно. Мы предполагаем, что для улучшения результата следует дать модели обучаться еще несколько эпох и сместить соотношение примеров в выборке в пользу тренеровочной, так как большее количество данных может позволить модели лучше дообучиться (мы делили на стандартные 80/20). Так же, можно было бы дотренеровать токенизатор и попробовать другие виды сэмплирования для валидации.

Примеры сгенерированных текстов можно найти в ноутбуках.
