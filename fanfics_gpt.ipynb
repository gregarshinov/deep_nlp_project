{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Finetune ruGPT3Small.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RHDK81QqrET"
      },
      "source": [
        "# Finetune ruGPT3Small on essays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK10D3MSpYty"
      },
      "source": [
        "## Install enviroment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asqMueYPeIgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c868257d-eb4a-4dca-c593-b1ae13b36da4"
      },
      "source": [
        "!pip3 install urllib3==1.25.4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting urllib3==1.25.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/0d/7777358f672a14b7ae0dfcd29f949f409f913e0578190d6bfa68eb55864b/urllib3-1.25.4-py2.py3-none-any.whl (125kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 19.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 26.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 31.6MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 27.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 29.6MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 32.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 71kB 26.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81kB 22.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 92kB 24.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 22.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 112kB 22.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 122kB 22.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 22.7MB/s \n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3\n",
            "  Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed urllib3-1.25.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPqtVgbkeTx7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d08fcf55-8ecf-49e3-c6a5-1f0250c2237b"
      },
      "source": [
        "!pip3 install transformers==2.8.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers==2.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/78/92cedda05552398352ed9784908b834ee32a0bd071a9b32de287327370b7/transformers-2.8.0-py3-none-any.whl (563kB)\n",
            "\r\u001b[K     |▋                               | 10kB 22.7MB/s eta 0:00:01\r\u001b[K     |█▏                              | 20kB 16.2MB/s eta 0:00:01\r\u001b[K     |█▊                              | 30kB 15.2MB/s eta 0:00:01\r\u001b[K     |██▎                             | 40kB 15.0MB/s eta 0:00:01\r\u001b[K     |███                             | 51kB 11.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 61kB 11.8MB/s eta 0:00:01\r\u001b[K     |████                            | 71kB 11.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 81kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 92kB 12.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 102kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 112kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 122kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 133kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 143kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 153kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 163kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 174kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 184kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 194kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 204kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 215kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 225kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 235kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 245kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 256kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 266kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 276kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 286kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 296kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 307kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 317kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 327kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 337kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 348kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 358kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 368kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 378kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 389kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 399kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 409kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 419kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 430kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 440kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 450kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 460kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 471kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 481kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 491kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 501kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 512kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 522kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 532kB 13.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 542kB 13.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 552kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 563kB 13.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 573kB 13.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (1.19.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (4.41.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 47.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (3.0.12)\n",
            "Collecting tokenizers==0.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/3f/73c881ea4723e43c1e9acf317cf407fab3a278daab3a69c98dcac511c04f/tokenizers-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 53.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.8.0) (0.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 52.5MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/bf/6fd00f2e8b2f9e8688b10b616c66be985a0053729cb1e92ac2f6e8ec1cd6/boto3-1.16.40-py2.py3-none-any.whl (130kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 55.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (1.25.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.8.0) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.8.0) (1.0.0)\n",
            "Collecting botocore<1.20.0,>=1.19.40\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/77/13fc099a10c22d08d766e244412c114694e21982c04cafc1ade33d8a430c/botocore-1.19.40-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 47.2MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.4.0,>=0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.40->boto3->transformers==2.8.0) (2.8.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=997f02c834cae77df6a96a06ab541a1ead9296f4d62233b9c2de669717c927d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, jmespath, botocore, s3transfer, boto3, transformers\n",
            "Successfully installed boto3-1.16.40 botocore-1.19.40 jmespath-0.10.0 s3transfer-0.3.3 sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.5.2 transformers-2.8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpkjTWefecLc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d94b679c-1bb6-45d5-d0f6-e1a3e93db1a8"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sberbank-ai/ru-gpts/master/pretrain_transformers.py"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-19 22:03:43--  https://raw.githubusercontent.com/sberbank-ai/ru-gpts/master/pretrain_transformers.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34150 (33K) [text/plain]\n",
            "Saving to: ‘pretrain_transformers.py’\n",
            "\n",
            "\rpretrain_transforme   0%[                    ]       0  --.-KB/s               \rpretrain_transforme 100%[===================>]  33.35K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-12-19 22:03:43 (140 MB/s) - ‘pretrain_transformers.py’ saved [34150/34150]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7zu3BqpqJQ7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85adce64-37af-4b6f-afbe-9ec7e0533240"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sberbank-ai/ru-gpts/master/generate_transformers.py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-19 22:03:47--  https://raw.githubusercontent.com/sberbank-ai/ru-gpts/master/generate_transformers.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10474 (10K) [text/plain]\n",
            "Saving to: ‘generate_transformers.py’\n",
            "\n",
            "\rgenerate_transforme   0%[                    ]       0  --.-KB/s               \rgenerate_transforme 100%[===================>]  10.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-12-19 22:03:47 (128 MB/s) - ‘generate_transformers.py’ saved [10474/10474]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlJB3Ln7gjO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79110592-ae59-4b1d-d399-ba61f894cbc1"
      },
      "source": [
        "%%writefile setup.sh\n",
        "\n",
        "export CUDA_HOME=/usr/local/cuda-10.1\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing setup.sh\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAZqpSEJglUl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29e2555-423e-4ae5-f752-5c899b2d6d89"
      },
      "source": [
        "!sh setup.sh"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'apex'...\n",
            "remote: Enumerating objects: 37, done.\u001b[K\n",
            "remote: Counting objects: 100% (37/37), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 7872 (delta 10), reused 7 (delta 0), pack-reused 7835\u001b[K\n",
            "Receiving objects: 100% (7872/7872), 13.99 MiB | 26.58 MiB/s, done.\n",
            "Resolving deltas: 100% (5357/5357), done.\n",
            "/usr/local/lib/python3.6/dist-packages/pip/_internal/commands/install.py:283: UserWarning: Disabling all use of wheels due to the use of --build-options / --global-options / --install-options.\n",
            "  cmdoptions.check_install_build_global(options)\n",
            "Created temporary directory: /tmp/pip-ephem-wheel-cache-ugn__lzb\n",
            "Created temporary directory: /tmp/pip-req-tracker-qcxtv2pt\n",
            "Created requirements tracker '/tmp/pip-req-tracker-qcxtv2pt'\n",
            "Created temporary directory: /tmp/pip-install-gcvahr66\n",
            "Processing ./apex\n",
            "  Created temporary directory: /tmp/pip-req-build-_nddd507\n",
            "  Added file:///content/apex to build tracker '/tmp/pip-req-tracker-qcxtv2pt'\n",
            "    Running setup.py (path:/tmp/pip-req-build-_nddd507/setup.py) egg_info for package from file:///content/apex\n",
            "    Running command python setup.py egg_info\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.7.0+cu101\n",
            "\n",
            "\n",
            "    running egg_info\n",
            "    creating /tmp/pip-req-build-_nddd507/pip-egg-info/apex.egg-info\n",
            "    writing /tmp/pip-req-build-_nddd507/pip-egg-info/apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to /tmp/pip-req-build-_nddd507/pip-egg-info/apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to /tmp/pip-req-build-_nddd507/pip-egg-info/apex.egg-info/top_level.txt\n",
            "    writing manifest file '/tmp/pip-req-build-_nddd507/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file '/tmp/pip-req-build-_nddd507/pip-egg-info/apex.egg-info/SOURCES.txt'\n",
            "    /tmp/pip-req-build-_nddd507/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "  Source in /tmp/pip-req-build-_nddd507 has version 0.1, which satisfies requirement apex==0.1 from file:///content/apex\n",
            "  Removed apex==0.1 from file:///content/apex from build tracker '/tmp/pip-req-tracker-qcxtv2pt'\n",
            "Skipping wheel build for apex, due to binaries being disabled for it.\n",
            "Installing collected packages: apex\n",
            "  Created temporary directory: /tmp/pip-record-du3tl6gi\n",
            "    Running command /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-req-build-_nddd507/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-req-build-_nddd507/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' --cpp_ext --cuda_ext install --record /tmp/pip-record-du3tl6gi/install-record.txt --single-version-externally-managed --compile\n",
            "\n",
            "\n",
            "    torch.__version__  = 1.7.0+cu101\n",
            "\n",
            "\n",
            "    /tmp/pip-req-build-_nddd507/setup.py:67: UserWarning: Option --pyprof not specified. Not installing PyProf dependencies!\n",
            "      warnings.warn(\"Option --pyprof not specified. Not installing PyProf dependencies!\")\n",
            "\n",
            "    Compiling cuda extensions with\n",
            "    nvcc: NVIDIA (R) Cuda compiler driver\n",
            "    Copyright (c) 2005-2019 NVIDIA Corporation\n",
            "    Built on Sun_Jul_28_19:07:16_PDT_2019\n",
            "    Cuda compilation tools, release 10.1, V10.1.243\n",
            "    from /usr/local/cuda-10.1/bin\n",
            "\n",
            "    running install\n",
            "    running build\n",
            "    running build_py\n",
            "    creating build\n",
            "    creating build/lib.linux-x86_64-3.6\n",
            "    creating build/lib.linux-x86_64-3.6/apex\n",
            "    copying apex/__init__.py -> build/lib.linux-x86_64-3.6/apex\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    copying apex/pyprof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/mlp.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    copying apex/mlp/__init__.py -> build/lib.linux-x86_64-3.6/apex/mlp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/LARC.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/distributed.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/multiproc.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm_kernel.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/__init__.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/optimized_sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    copying apex/parallel/sync_batchnorm.py -> build/lib.linux-x86_64-3.6/apex/parallel\n",
            "    creating build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/reparameterization.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/weight_norm.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    copying apex/reparameterization/__init__.py -> build/lib.linux-x86_64-3.6/apex/reparameterization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/fused_layer_norm.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    copying apex/normalization/__init__.py -> build/lib.linux-x86_64-3.6/apex/normalization\n",
            "    creating build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/loss_scaler.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/__init__.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    copying apex/fp16_utils/fp16util.py -> build/lib.linux-x86_64-3.6/apex/fp16_utils\n",
            "    creating build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_novograd.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    copying apex/optimizers/fused_adagrad.py -> build/lib.linux-x86_64-3.6/apex/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/cells.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/models.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/__init__.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    copying apex/RNN/RNNBackend.py -> build/lib.linux-x86_64-3.6/apex/RNN\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    copying apex/contrib/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/rnn_compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/frontend.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/utils.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_process_optimizer.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/wrap.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__version__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_initialize.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/_amp_state.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/compat.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/handle.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/amp.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/opt.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    copying apex/amp/scaler.py -> build/lib.linux-x86_64-3.6/apex/amp\n",
            "    creating build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/multi_tensor_apply.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    copying apex/multi_tensor_apply/__init__.py -> build/lib.linux-x86_64-3.6/apex/multi_tensor_apply\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/recurrentCell.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/index_slice_join_mutate.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/output.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/utility.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/misc.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/base.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/blas.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/normalization.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/convert.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pointwise.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/prof.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/softmax.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/optim.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/reduction.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/data.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/randomSample.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/linear.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/loss.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/pooling.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/conv.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/embedding.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/dropout.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/activation.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    copying apex/pyprof/prof/usage.py -> build/lib.linux-x86_64-3.6/apex/pyprof/prof\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/nvmarker.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    copying apex/pyprof/nvtx/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/nvtx\n",
            "    creating build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__main__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/parse.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/kernel.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/__init__.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/nvvp.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    copying apex/pyprof/parse/db.py -> build/lib.linux-x86_64-3.6/apex/pyprof/parse\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v2.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_sgd.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fp16_optimizer.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_adam_v3.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/fused_adam.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    copying apex/contrib/optimizers/distributed_fused_lamb.py -> build/lib.linux-x86_64-3.6/apex/contrib/optimizers\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/batch_norm.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    copying apex/contrib/groupbn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/groupbn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    copying apex/contrib/multihead_attn/self_multihead_attn_func.py -> build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    copying apex/contrib/xentropy/softmax_xentropy.py -> build/lib.linux-x86_64-3.6/apex/contrib/xentropy\n",
            "    creating build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/sparse_masklib.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/__init__.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    copying apex/contrib/sparsity/asp.py -> build/lib.linux-x86_64-3.6/apex/contrib/sparsity\n",
            "    creating build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/tensor_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/functional_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/__init__.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    copying apex/amp/lists/torch_overrides.py -> build/lib.linux-x86_64-3.6/apex/amp/lists\n",
            "    running build_ext\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/utils/cpp_extension.py:339: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
            "      warnings.warn(msg.format('we could not find ninja.'))\n",
            "    building 'apex_C' extension\n",
            "    creating build/temp.linux-x86_64-3.6\n",
            "    creating build/temp.linux-x86_64-3.6/csrc\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/include/python3.6m -c csrc/flatten_unflatten.cpp -o build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=apex_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    In file included from csrc/flatten_unflatten.cpp:2:0:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h: In member function ‘at::DeprecatedTypeProperties& torch::utils::TensorGroup::type()’:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/utils/tensor_flatten.h:36:28: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         return tensors[0].type();\n",
            "                                ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/flatten_unflatten.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/flatten_unflatten.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'amp_C' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/amp_C_frontend.cpp -o build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/amp_C_frontend.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_sgd_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_scale_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_axpby_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_l2norm_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_1.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb_stage_2.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_adam.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_adagrad.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_novograd.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/multi_tensor_lamb.cu -o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -lineinfo -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=amp_C -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/amp_C_frontend.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_sgd_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_scale_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_axpby_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_l2norm_kernel.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_1.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb_stage_2.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adam.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_adagrad.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_novograd.o build/temp.linux-x86_64-3.6/csrc/multi_tensor_lamb.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'syncbn' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/syncbn.cpp -o build/temp.linux-x86_64-3.6/csrc/syncbn.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/syncbn.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/welford.cu -o build/temp.linux-x86_64-3.6/csrc/welford.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=syncbn -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/syncbn.o build/temp.linux-x86_64-3.6/csrc/welford.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'fused_layer_norm_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda.cpp -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm(at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:129:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_affine(at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:149:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:150:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:151:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘at::Tensor layer_norm_gradient(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:193:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:194:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:195:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:196:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp: In function ‘std::vector<at::Tensor> layer_norm_gradient_affine(at::Tensor, at::Tensor, at::Tensor, at::Tensor, c10::IntArrayRef, at::Tensor, at::Tensor, double)’:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:218:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(dout);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:219:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(mean);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:220:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(invvar);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:221:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(input);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:222:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(gamma);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/DeviceType.h:8:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Device.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/c10/core/Allocator.h:6,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:7,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    csrc/layer_norm_cuda.cpp:117:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                                              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/macros/Macros.h:171:65: note: in definition of macro ‘C10_UNLIKELY’\n",
            "     #define C10_UNLIKELY(expr)  (__builtin_expect(static_cast<bool>(expr), 0))\n",
            "                                                                     ^~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:330:7: note: in expansion of macro ‘C10_UNLIKELY_OR_CONST’\n",
            "       if (C10_UNLIKELY_OR_CONST(!(cond))) {                               \\\n",
            "           ^~~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:318:3: note: in expansion of macro ‘TORCH_CHECK_WITH_MSG’\n",
            "       TORCH_CHECK_WITH_MSG(error_t, cond, \"\", __VA_ARGS__)\n",
            "       ^~~~~~~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/c10/util/Exception.h:341:32: note: in expansion of macro ‘TORCH_CHECK_WITH’\n",
            "     #define TORCH_CHECK(cond, ...) TORCH_CHECK_WITH(Error, cond, __VA_ARGS__)\n",
            "                                    ^~~~~~~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:117:23: note: in expansion of macro ‘TORCH_CHECK’\n",
            "     #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:119:24: note: in expansion of macro ‘CHECK_CUDA’\n",
            "     #define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
            "                            ^~~~~~~~~~\n",
            "    csrc/layer_norm_cuda.cpp:223:3: note: in expansion of macro ‘CHECK_INPUT’\n",
            "       CHECK_INPUT(beta);\n",
            "       ^~~~~~~~~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/layer_norm_cuda.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/layer_norm_cuda_kernel.cu -o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -maxrregcount=50 -O3 --use_fast_math -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=fused_layer_norm_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda.o build/temp.linux-x86_64-3.6/csrc/layer_norm_cuda_kernel.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    building 'mlp_cuda' extension\n",
            "    x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/mlp.cpp -o build/temp.linux-x86_64-3.6/csrc/mlp.o -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Parallel.h:149:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/utils.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn/cloneable.h:5,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/nn.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:12,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ParallelOpenMP.h:84:0: warning: ignoring #pragma omp parallel [-Wunknown-pragmas]\n",
            "     #pragma omp parallel for if ((end - begin) >= grain_size)\n",
            "\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_forward(int, int, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:56:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:64:77: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto out = at::empty({batch_size, output_features.back()}, inputs[0].type());\n",
            "                                                                                 ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "       auto reserved_space = at::empty({reserved_size}, inputs[0].type());\n",
            "                                                                        ^\n",
            "    csrc/mlp.cpp:65:68: warning: narrowing conversion of ‘reserved_size’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:67:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:70:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:76:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_fp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:67:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_forward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In function ‘std::vector<at::Tensor> mlp_backward(int, int, at::Tensor, std::vector<at::Tensor>, std::vector<at::Tensor>)’:\n",
            "    csrc/mlp.cpp:113:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < num_layers; i++) {\n",
            "                       ~~^~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:119:21: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "       for (int i = 0; i < inputs.size(); i++) {\n",
            "                       ~~^~~~~~~~~~~~~~~\n",
            "    csrc/mlp.cpp:120:67: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         outputs.push_back(at::empty(inputs[i].sizes(), inputs[i].type()));  // clone for testing now\n",
            "                                                                       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:123:54: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "                                                          ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:150:28: note: in definition of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "         const auto& the_type = TYPE;                                            \\\n",
            "                                ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:152:56: warning: ‘c10::ScalarType detail::scalar_type(const at::DeprecatedTypeProperties&)’ is deprecated: passing at::DeprecatedTypeProperties to an AT_DISPATCH macro is deprecated, pass an at::ScalarType instead [-Wdeprecated-declarations]\n",
            "         at::ScalarType _st = ::detail::scalar_type(the_type);                   \\\n",
            "                                                            ^\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:66:23: note: declared here\n",
            "     inline at::ScalarType scalar_type(const at::DeprecatedTypeProperties& t) {\n",
            "                           ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp: In lambda function:\n",
            "    csrc/mlp.cpp:125:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < num_layers; i++) {\n",
            "                         ~~^~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:129:23: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
            "         for (int i = 0; i < inputs.size(); i++) {\n",
            "                         ~~^~~~~~~~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:80: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                                                                    ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Tensor.h:3:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Context.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:9,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/TensorBody.h:277:30: note: declared here\n",
            "       DeprecatedTypeProperties & type() const {\n",
            "                                  ^~~~\n",
            "    In file included from /usr/local/lib/python3.6/dist-packages/torch/include/ATen/ATen.h:13:0,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/types.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader_options.h:4,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/base.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader/stateful.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data/dataloader.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/data.h:3,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include/torch/all.h:8,\n",
            "                     from /usr/local/lib/python3.6/dist-packages/torch/include/torch/extension.h:4,\n",
            "                     from csrc/mlp.cpp:1:\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:137:44: warning: narrowing conversion of ‘(work_size / sizeof (scalar_t))’ from ‘long unsigned int’ to ‘long int’ inside { } [-Wnarrowing]\n",
            "         auto work_space = at::empty({work_size / sizeof(scalar_t)}, inputs[0].type());\n",
            "                                      ~~~~~~~~~~^~~~~~~~~\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    csrc/mlp.cpp:139:10: warning: unused variable ‘result’ [-Wunused-variable]\n",
            "         auto result = mlp_bp<scalar_t>(\n",
            "              ^\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/Dispatch.h:13:12: note: in definition of macro ‘AT_PRIVATE_CASE_TYPE’\n",
            "         return __VA_ARGS__();                          \\\n",
            "                ^~~~~~~~~~~\n",
            "    csrc/mlp.cpp:123:3: note: in expansion of macro ‘AT_DISPATCH_FLOATING_TYPES_AND_HALF’\n",
            "       AT_DISPATCH_FLOATING_TYPES_AND_HALF(inputs[0].type(), \"mlp_backward\", [&] {\n",
            "       ^\n",
            "    /usr/local/cuda-10.1/bin/nvcc -I/usr/local/lib/python3.6/dist-packages/torch/include -I/usr/local/lib/python3.6/dist-packages/torch/include/torch/csrc/api/include -I/usr/local/lib/python3.6/dist-packages/torch/include/TH -I/usr/local/lib/python3.6/dist-packages/torch/include/THC -I/usr/local/cuda-10.1/include -I/usr/include/python3.6m -c csrc/mlp_cuda.cu -o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr --compiler-options '-fPIC' -O3 -DVERSION_GE_1_1 -DVERSION_GE_1_3 -DVERSION_GE_1_5 -DTORCH_API_INCLUDE_EXTENSION_H -DTORCH_EXTENSION_NAME=mlp_cuda -D_GLIBCXX_USE_CXX11_ABI=0 -gencode=arch=compute_75,code=sm_75 -std=c++14\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/boxing/impl/boxing.h(100): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    /usr/local/lib/python3.6/dist-packages/torch/include/ATen/core/op_registration/op_whitelist.h(39): warning: integer conversion resulted in a change of sign\n",
            "\n",
            "    x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.6/csrc/mlp.o build/temp.linux-x86_64-3.6/csrc/mlp_cuda.o -L/usr/local/lib/python3.6/dist-packages/torch/lib -L/usr/local/cuda-10.1/lib64 -lc10 -ltorch -ltorch_cpu -ltorch_python -lcudart -lc10_cuda -ltorch_cuda -o build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so\n",
            "    running install_lib\n",
            "    copying build/lib.linux-x86_64-3.6/mlp_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/syncbn.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/apex_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/recurrentCell.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/index_slice_join_mutate.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/output.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/utility.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/misc.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/base.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/blas.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/normalization.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/convert.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pointwise.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/prof.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/softmax.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/optim.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/reduction.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/data.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/randomSample.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/linear.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/loss.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/pooling.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/conv.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/embedding.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/dropout.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/activation.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/prof/usage.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/nvmarker.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/nvtx/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__main__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/parse.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/nvvp.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    copying build/lib.linux-x86_64-3.6/apex/pyprof/parse/db.py -> /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/mlp.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/mlp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/mlp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/LARC.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/distributed.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/multiproc.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm_kernel.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/optimized_sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    copying build/lib.linux-x86_64-3.6/apex/parallel/sync_batchnorm.py -> /usr/local/lib/python3.6/dist-packages/apex/parallel\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/reparameterization.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/weight_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/reparameterization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/reparameterization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/fused_layer_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    copying build/lib.linux-x86_64-3.6/apex/normalization/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/normalization\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/loss_scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    copying build/lib.linux-x86_64-3.6/apex/fp16_utils/fp16util.py -> /usr/local/lib/python3.6/dist-packages/apex/fp16_utils\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_novograd.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/optimizers/fused_adagrad.py -> /usr/local/lib/python3.6/dist-packages/apex/optimizers\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/cells.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/models.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    copying build/lib.linux-x86_64-3.6/apex/RNN/RNNBackend.py -> /usr/local/lib/python3.6/dist-packages/apex/RNN\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v2.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_sgd.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fp16_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_adam_v3.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/fused_adam.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/optimizers/distributed_fused_lamb.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/batch_norm.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/groupbn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/mask_softmax_dropout_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/multihead_attn/self_multihead_attn_func.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/xentropy/softmax_xentropy.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/sparse_masklib.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/contrib/sparsity/asp.py -> /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity\n",
            "    copying build/lib.linux-x86_64-3.6/apex/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/rnn_compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/frontend.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/utils.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_process_optimizer.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/wrap.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__version__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/tensor_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/functional_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/lists/torch_overrides.py -> /usr/local/lib/python3.6/dist-packages/apex/amp/lists\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_initialize.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/_amp_state.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/compat.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/handle.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/amp.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/opt.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    copying build/lib.linux-x86_64-3.6/apex/amp/scaler.py -> /usr/local/lib/python3.6/dist-packages/apex/amp\n",
            "    creating /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/multi_tensor_apply.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/apex/multi_tensor_apply/__init__.py -> /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply\n",
            "    copying build/lib.linux-x86_64-3.6/fused_layer_norm_cuda.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    copying build/lib.linux-x86_64-3.6/amp_C.cpython-36m-x86_64-linux-gnu.so -> /usr/local/lib/python3.6/dist-packages\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/recurrentCell.py to recurrentCell.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/index_slice_join_mutate.py to index_slice_join_mutate.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/output.py to output.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/utility.py to utility.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/misc.py to misc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/base.py to base.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/blas.py to blas.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/normalization.py to normalization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/convert.py to convert.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pointwise.py to pointwise.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/prof.py to prof.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/softmax.py to softmax.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/optim.py to optim.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/reduction.py to reduction.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/data.py to data.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/randomSample.py to randomSample.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/linear.py to linear.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/loss.py to loss.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/pooling.py to pooling.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/conv.py to conv.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/embedding.py to embedding.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/dropout.py to dropout.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/activation.py to activation.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/prof/usage.py to usage.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/nvmarker.py to nvmarker.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/nvtx/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__main__.py to __main__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/parse.py to parse.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/kernel.py to kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/nvvp.py to nvvp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/pyprof/parse/db.py to db.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/mlp.py to mlp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/mlp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/LARC.py to LARC.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/distributed.py to distributed.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm_kernel.py to optimized_sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/multiproc.py to multiproc.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm_kernel.py to sync_batchnorm_kernel.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/optimized_sync_batchnorm.py to optimized_sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/parallel/sync_batchnorm.py to sync_batchnorm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/reparameterization.py to reparameterization.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/weight_norm.py to weight_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/reparameterization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/fused_layer_norm.py to fused_layer_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/normalization/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/loss_scaler.py to loss_scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/fp16_utils/fp16util.py to fp16util.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_novograd.py to fused_novograd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/optimizers/fused_adagrad.py to fused_adagrad.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/cells.py to cells.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/models.py to models.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/RNN/RNNBackend.py to RNNBackend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v2.py to distributed_fused_adam_v2.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_sgd.py to fused_sgd.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fp16_optimizer.py to fp16_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_lamb.py to fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam.py to distributed_fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_adam_v3.py to distributed_fused_adam_v3.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/fused_adam.py to fused_adam.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/optimizers/distributed_fused_lamb.py to distributed_fused_lamb.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/batch_norm.py to batch_norm.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/groupbn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn.py to encdec_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/encdec_multihead_attn_func.py to encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_func.py to fast_encdec_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_norm_add_func.py to fast_self_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_encdec_multihead_attn_norm_add_func.py to fast_encdec_multihead_attn_norm_add_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/fast_self_multihead_attn_func.py to fast_self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/mask_softmax_dropout_func.py to mask_softmax_dropout_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn.py to self_multihead_attn.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/multihead_attn/self_multihead_attn_func.py to self_multihead_attn_func.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/xentropy/softmax_xentropy.py to softmax_xentropy.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/sparse_masklib.py to sparse_masklib.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/contrib/sparsity/asp.py to asp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/rnn_compat.py to rnn_compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/frontend.py to frontend.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/utils.py to utils.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_process_optimizer.py to _process_optimizer.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py to wrap.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__version__.py to __version__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/tensor_overrides.py to tensor_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/functional_overrides.py to functional_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/lists/torch_overrides.py to torch_overrides.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_initialize.py to _initialize.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/_amp_state.py to _amp_state.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/compat.py to compat.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/__init__.py to __init__.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/handle.py to handle.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/amp.py to amp.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/opt.py to opt.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/amp/scaler.py to scaler.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/multi_tensor_apply.py to multi_tensor_apply.cpython-36.pyc\n",
            "    byte-compiling /usr/local/lib/python3.6/dist-packages/apex/multi_tensor_apply/__init__.py to __init__.cpython-36.pyc\n",
            "    running install_egg_info\n",
            "    running egg_info\n",
            "    creating apex.egg-info\n",
            "    writing apex.egg-info/PKG-INFO\n",
            "    writing dependency_links to apex.egg-info/dependency_links.txt\n",
            "    writing top-level names to apex.egg-info/top_level.txt\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    writing manifest file 'apex.egg-info/SOURCES.txt'\n",
            "    Copying apex.egg-info to /usr/local/lib/python3.6/dist-packages/apex-0.1-py3.6.egg-info\n",
            "    running install_scripts\n",
            "    writing list of installed files to '/tmp/pip-record-du3tl6gi/install-record.txt'\n",
            "    Running setup.py install for apex ... \u001b[?25l\u001b[?25hdone\n",
            "  Removing source in /tmp/pip-req-build-_nddd507\n",
            "Successfully installed apex-0.1\n",
            "Cleaning up...\n",
            "Removed build tracker '/tmp/pip-req-tracker-qcxtv2pt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP7YAlDPqknI"
      },
      "source": [
        "## Add data to colab\n",
        "Add essays file from google dirve:\n",
        "* Add [file](https://drive.google.com/file/d/10ZsjTeaoihYA80n1G40O5YZmaGw0yOXk/view?usp=sharing) to your own drive\n",
        "* And mount drive to colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTN7lA4BqbRz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da801197-5345-4d4f-8e0d-92701754935a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5EtK-jerBRv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e9d1e07-b46e-4a89-9540-d7af8d99dc20"
      },
      "source": [
        "data_path = \"drive/My Drive/essays.txt\"\n",
        "!ls \"$data_path\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'drive/My Drive/essays.txt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDYi1TVTrtkO"
      },
      "source": [
        "## Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXdNbrq3rgzq"
      },
      "source": [
        "with open(data_path, \"r\") as file:\n",
        "    text = file.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sgqpozwryu_"
      },
      "source": [
        "valid_size = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5XsmNzor0pK"
      },
      "source": [
        "topics = []\n",
        "all_essays = []\n",
        "for line in text.split(\"</s>\"):\n",
        "    if \"Тема:\" in line and \"Сочинение:\" in line:\n",
        "        essay_text = line.split(\"Сочинение:\")\n",
        "        if len(essay_text) == 2:\n",
        "            topic = essay_text[0].replace(\"<s>\", \" \").replace(\"</s>\", \" \").strip()\n",
        "            essay_text = essay_text[1].replace(\"<s>\", \" \").replace(\"</s>\", \" \").strip()\n",
        "            essay_text = f\"Сочинение: {essay_text}\"\n",
        "            essay_res = f\"<s>{topic}\\n{essay_text}</s>\"\n",
        "            all_essays.append(essay_res)\n",
        "            topics.append(topic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izc0lkkHr2Rz"
      },
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tkc1_49r36X"
      },
      "source": [
        "random.seed(1234)\n",
        "np.random.seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EHAdpt5r5B9"
      },
      "source": [
        "unique_topics = list(set(topics))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUfoak6Hr6Ed"
      },
      "source": [
        "valid_topics = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x55W6Xf4r7MK"
      },
      "source": [
        "for _ in range(valid_size):\n",
        "    # Use randint for more speed (on big lists it is faster)\n",
        "    idx = np.random.randint(0, len(unique_topics))\n",
        "    valid_topics.append(unique_topics[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "861f4lMGr8ez"
      },
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "train = []\n",
        "valid = []\n",
        "for topic, essay in zip(topics, all_essays):\n",
        "    is_train = True\n",
        "    for valid_topic in valid_topics:\n",
        "        if (\n",
        "            nltk.edit_distance(valid_topic, topic[:len(valid_topic)]) < 20 or\n",
        "            nltk.edit_distance(valid_topic[:len(topic)], topic) < 20 or\n",
        "            nltk.edit_distance(valid_topic[len(topic):], topic) < 20 or\n",
        "            nltk.edit_distance(valid_topic, topic[len(valid_topic):]) < 20\n",
        "            ):\n",
        "            is_train = False\n",
        "    if is_train:\n",
        "        train.append(essay)\n",
        "    else:\n",
        "        valid.append(essay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2T0gN6gqr9pa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57e386ee-3180-4e7a-ec7b-df98e8b67125"
      },
      "source": [
        "len(train), len(valid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(313, 87)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPB8rrVPr-kh"
      },
      "source": [
        "with open(\"train.txt\", \"w\") as file:\n",
        "    file.write(\"\\n\".join(train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP5_nk_0sAB0"
      },
      "source": [
        "with open(\"valid.txt\", \"w\") as file:\n",
        "    file.write(\"\\n\".join(valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NitGcEKPsDQE"
      },
      "source": [
        "## Run finetuning\n",
        "The following code download our model and tokenizer from transformers and finetune model essays.\n",
        "\n",
        "This took aroung ten minutes and obtain perplexity = 13-16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vL07XFvsBBU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2876ceaf-e80b-48cc-db8a-108d01bde60b"
      },
      "source": [
        "!python pretrain_transformers.py \\\n",
        "    --output_dir=essays_model \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=sberbank-ai/rugpt3small_based_on_gpt2 \\\n",
        "    --do_train \\\n",
        "    --train_data_file=train.txt \\\n",
        "    --do_eval \\\n",
        "    --fp16 \\\n",
        "    --eval_data_file=val.txt \\\n",
        "    --per_gpu_train_batch_size 1 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --num_train_epochs 5 \\\n",
        "    --block_size 2048 \\\n",
        "    --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-19 23:14:51.629030: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "12/19/2020 23:14:54 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: True\n",
            "12/19/2020 23:14:54 - INFO - filelock -   Lock 140204873948072 acquired on /root/.cache/torch/transformers/767f65149d3a8095b8ac0370c3b2bbc7f05d863ff413b010d2f5206de0349fd0.1ede3f500f8b09312434582244c28d34f008436c5a38bb3afa159d2b38f8e758.lock\n",
            "12/19/2020 23:14:54 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/config.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp0zcoznhx\n",
            "Downloading: 100% 608/608 [00:00<00:00, 726kB/s]\n",
            "12/19/2020 23:14:54 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/config.json in cache at /root/.cache/torch/transformers/767f65149d3a8095b8ac0370c3b2bbc7f05d863ff413b010d2f5206de0349fd0.1ede3f500f8b09312434582244c28d34f008436c5a38bb3afa159d2b38f8e758\n",
            "12/19/2020 23:14:54 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/767f65149d3a8095b8ac0370c3b2bbc7f05d863ff413b010d2f5206de0349fd0.1ede3f500f8b09312434582244c28d34f008436c5a38bb3afa159d2b38f8e758\n",
            "12/19/2020 23:14:54 - INFO - filelock -   Lock 140204873948072 released on /root/.cache/torch/transformers/767f65149d3a8095b8ac0370c3b2bbc7f05d863ff413b010d2f5206de0349fd0.1ede3f500f8b09312434582244c28d34f008436c5a38bb3afa159d2b38f8e758.lock\n",
            "12/19/2020 23:14:54 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/config.json from cache at /root/.cache/torch/transformers/767f65149d3a8095b8ac0370c3b2bbc7f05d863ff413b010d2f5206de0349fd0.1ede3f500f8b09312434582244c28d34f008436c5a38bb3afa159d2b38f8e758\n",
            "12/19/2020 23:14:54 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 2,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "12/19/2020 23:14:54 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/config.json from cache at /root/.cache/torch/transformers/767f65149d3a8095b8ac0370c3b2bbc7f05d863ff413b010d2f5206de0349fd0.1ede3f500f8b09312434582244c28d34f008436c5a38bb3afa159d2b38f8e758\n",
            "12/19/2020 23:14:54 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 2,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50264\n",
            "}\n",
            "\n",
            "12/19/2020 23:14:54 - INFO - transformers.tokenization_utils -   Model name 'sberbank-ai/rugpt3small_based_on_gpt2' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'sberbank-ai/rugpt3small_based_on_gpt2' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "12/19/2020 23:14:54 - INFO - filelock -   Lock 140204873945328 acquired on /root/.cache/torch/transformers/5a758d8ad17bbe5e01facf6fdc06ccd64f58fddc8507d568d68db3926c013e35.de52bc5880aff0437c7f24c33b71ecae48f6f03f0449dfe933503132c6c1cc26.lock\n",
            "12/19/2020 23:14:54 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/vocab.json not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmp1uazu639\n",
            "Downloading: 100% 1.71M/1.71M [00:00<00:00, 41.3MB/s]\n",
            "12/19/2020 23:14:54 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/vocab.json in cache at /root/.cache/torch/transformers/5a758d8ad17bbe5e01facf6fdc06ccd64f58fddc8507d568d68db3926c013e35.de52bc5880aff0437c7f24c33b71ecae48f6f03f0449dfe933503132c6c1cc26\n",
            "12/19/2020 23:14:54 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/5a758d8ad17bbe5e01facf6fdc06ccd64f58fddc8507d568d68db3926c013e35.de52bc5880aff0437c7f24c33b71ecae48f6f03f0449dfe933503132c6c1cc26\n",
            "12/19/2020 23:14:54 - INFO - filelock -   Lock 140204873945328 released on /root/.cache/torch/transformers/5a758d8ad17bbe5e01facf6fdc06ccd64f58fddc8507d568d68db3926c013e35.de52bc5880aff0437c7f24c33b71ecae48f6f03f0449dfe933503132c6c1cc26.lock\n",
            "12/19/2020 23:14:54 - INFO - filelock -   Lock 140204873907392 acquired on /root/.cache/torch/transformers/f90e02223dfb7632579f3421c7e97b27e60a2ebc5462220cfedb252a4e1893a8.5885500c9887f152893bfadf3b511a9105243c57bfc45889e3552bdc61090032.lock\n",
            "12/19/2020 23:14:54 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/merges.txt not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpz7ff2w4s\n",
            "Downloading: 100% 1.27M/1.27M [00:00<00:00, 34.8MB/s]\n",
            "12/19/2020 23:14:54 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/merges.txt in cache at /root/.cache/torch/transformers/f90e02223dfb7632579f3421c7e97b27e60a2ebc5462220cfedb252a4e1893a8.5885500c9887f152893bfadf3b511a9105243c57bfc45889e3552bdc61090032\n",
            "12/19/2020 23:14:54 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/f90e02223dfb7632579f3421c7e97b27e60a2ebc5462220cfedb252a4e1893a8.5885500c9887f152893bfadf3b511a9105243c57bfc45889e3552bdc61090032\n",
            "12/19/2020 23:14:54 - INFO - filelock -   Lock 140204873907392 released on /root/.cache/torch/transformers/f90e02223dfb7632579f3421c7e97b27e60a2ebc5462220cfedb252a4e1893a8.5885500c9887f152893bfadf3b511a9105243c57bfc45889e3552bdc61090032.lock\n",
            "12/19/2020 23:14:54 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/vocab.json from cache at /root/.cache/torch/transformers/5a758d8ad17bbe5e01facf6fdc06ccd64f58fddc8507d568d68db3926c013e35.de52bc5880aff0437c7f24c33b71ecae48f6f03f0449dfe933503132c6c1cc26\n",
            "12/19/2020 23:14:54 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/merges.txt from cache at /root/.cache/torch/transformers/f90e02223dfb7632579f3421c7e97b27e60a2ebc5462220cfedb252a4e1893a8.5885500c9887f152893bfadf3b511a9105243c57bfc45889e3552bdc61090032\n",
            "12/19/2020 23:14:54 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/added_tokens.json from cache at None\n",
            "12/19/2020 23:14:54 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/special_tokens_map.json from cache at None\n",
            "12/19/2020 23:14:54 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/tokenizer_config.json from cache at None\n",
            "12/19/2020 23:14:54 - INFO - filelock -   Lock 140204873948464 acquired on /root/.cache/torch/transformers/dd2ab9d5315e399136239fcbfb166f5bc068648d56d0bd6020de1d7015822b29.bd2a4d97a415a80a6bce0491f034d891869f2a2121c7fc8f2047c42d56494b3d.lock\n",
            "12/19/2020 23:14:54 - INFO - transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/torch/transformers/tmpk91tnanv\n",
            "Downloading: 100% 551M/551M [00:07<00:00, 71.7MB/s]\n",
            "12/19/2020 23:15:02 - INFO - transformers.file_utils -   storing https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/pytorch_model.bin in cache at /root/.cache/torch/transformers/dd2ab9d5315e399136239fcbfb166f5bc068648d56d0bd6020de1d7015822b29.bd2a4d97a415a80a6bce0491f034d891869f2a2121c7fc8f2047c42d56494b3d\n",
            "12/19/2020 23:15:02 - INFO - transformers.file_utils -   creating metadata file for /root/.cache/torch/transformers/dd2ab9d5315e399136239fcbfb166f5bc068648d56d0bd6020de1d7015822b29.bd2a4d97a415a80a6bce0491f034d891869f2a2121c7fc8f2047c42d56494b3d\n",
            "12/19/2020 23:15:02 - INFO - filelock -   Lock 140204873948464 released on /root/.cache/torch/transformers/dd2ab9d5315e399136239fcbfb166f5bc068648d56d0bd6020de1d7015822b29.bd2a4d97a415a80a6bce0491f034d891869f2a2121c7fc8f2047c42d56494b3d.lock\n",
            "12/19/2020 23:15:02 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/sberbank-ai/rugpt3small_based_on_gpt2/pytorch_model.bin from cache at /root/.cache/torch/transformers/dd2ab9d5315e399136239fcbfb166f5bc068648d56d0bd6020de1d7015822b29.bd2a4d97a415a80a6bce0491f034d891869f2a2121c7fc8f2047c42d56494b3d\n",
            "12/19/2020 23:15:08 - INFO - transformers.modeling_utils -   Weights from pretrained model not used in GPT2LMHeadModel: ['transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.masked_bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.masked_bias']\n",
            "12/19/2020 23:15:22 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=2048, cache_dir=None, config_name=None, device=device(type='cuda'), do_eval=True, do_train=True, eval_all_checkpoints=False, eval_data_file='val.txt', evaluate_during_training=False, fp16=True, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, line_by_line=False, local_rank=-1, logging_steps=500, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='sberbank-ai/rugpt3small_based_on_gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=5.0, output_dir='essays_model', overwrite_cache=False, overwrite_output_dir=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=1, save_steps=500, save_total_limit=None, seed=42, server_ip='', server_port='', should_continue=False, tokenizer_name=None, train_data_file='train.txt', warmup_steps=0, weight_decay=0.01)\n",
            "12/19/2020 23:15:22 - INFO - __main__ -   Creating features from dataset file at \n",
            "12/19/2020 23:15:45 - INFO - __main__ -   Saving features into cached file gpt2_cached_lm_2048_train.txt\n",
            "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O1\n",
            "cast_model_type        : None\n",
            "patch_torch_functions  : True\n",
            "keep_batchnorm_fp32    : None\n",
            "master_weights         : None\n",
            "loss_scale             : dynamic\n",
            "12/19/2020 23:15:45 - INFO - __main__ -   ***** Running training *****\n",
            "12/19/2020 23:15:45 - INFO - __main__ -     Num examples = 947\n",
            "12/19/2020 23:15:45 - INFO - __main__ -     Num Epochs = 5\n",
            "12/19/2020 23:15:45 - INFO - __main__ -     Instantaneous batch size per GPU = 1\n",
            "12/19/2020 23:15:45 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "12/19/2020 23:15:45 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "12/19/2020 23:15:45 - INFO - __main__ -     Total optimization steps = 4735\n",
            "Epoch:   0% 0/5 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/947 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/apex/amp/wrap.py:101: UserWarning: This overload of add_ is deprecated:\n",
            "\tadd_(Number alpha, Tensor other)\n",
            "Consider using one of the following signatures instead:\n",
            "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
            "  return orig_fn(arg0, *args, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "\n",
            "Iteration:   0% 1/947 [00:01<23:16,  1.48s/it]\u001b[A\n",
            "Iteration:   0% 2/947 [00:02<19:05,  1.21s/it]\u001b[A\n",
            "Iteration:   0% 3/947 [00:02<16:14,  1.03s/it]\u001b[A\n",
            "Iteration:   0% 4/947 [00:03<14:14,  1.10it/s]\u001b[A\n",
            "Iteration:   1% 5/947 [00:03<12:51,  1.22it/s]\u001b[A\n",
            "Iteration:   1% 6/947 [00:04<11:50,  1.32it/s]\u001b[A\n",
            "Iteration:   1% 7/947 [00:05<11:10,  1.40it/s]\u001b[A\n",
            "Iteration:   1% 8/947 [00:05<10:39,  1.47it/s]\u001b[A\n",
            "Iteration:   1% 9/947 [00:06<10:19,  1.51it/s]\u001b[A\n",
            "Iteration:   1% 10/947 [00:06<10:05,  1.55it/s]\u001b[A\n",
            "Iteration:   1% 11/947 [00:07<09:54,  1.58it/s]\u001b[A\n",
            "Iteration:   1% 12/947 [00:08<09:46,  1.59it/s]\u001b[A\n",
            "Iteration:   1% 13/947 [00:08<09:41,  1.61it/s]\u001b[A\n",
            "Iteration:   1% 14/947 [00:09<09:37,  1.61it/s]\u001b[A\n",
            "Iteration:   2% 15/947 [00:10<09:35,  1.62it/s]\u001b[A\n",
            "Iteration:   2% 16/947 [00:10<09:33,  1.62it/s]\u001b[A\n",
            "Iteration:   2% 17/947 [00:11<09:32,  1.63it/s]\u001b[A\n",
            "Iteration:   2% 18/947 [00:11<09:30,  1.63it/s]\u001b[A\n",
            "Iteration:   2% 19/947 [00:12<09:29,  1.63it/s]\u001b[AGradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
            "\n",
            "Iteration:   2% 20/947 [00:13<09:22,  1.65it/s]\u001b[A\n",
            "Iteration:   2% 21/947 [00:13<09:17,  1.66it/s]\u001b[A\n",
            "Iteration:   2% 22/947 [00:14<09:19,  1.65it/s]\u001b[A\n",
            "Iteration:   2% 23/947 [00:14<09:21,  1.65it/s]\u001b[A\n",
            "Iteration:   3% 24/947 [00:15<09:24,  1.63it/s]\u001b[A\n",
            "Iteration:   3% 25/947 [00:16<09:22,  1.64it/s]\u001b[A\n",
            "Iteration:   3% 26/947 [00:16<09:23,  1.63it/s]\u001b[A\n",
            "Iteration:   3% 27/947 [00:17<09:23,  1.63it/s]\u001b[A\n",
            "Iteration:   3% 28/947 [00:17<09:23,  1.63it/s]\u001b[A\n",
            "Iteration:   3% 29/947 [00:18<09:22,  1.63it/s]\u001b[A\n",
            "Iteration:   3% 30/947 [00:19<09:23,  1.63it/s]\u001b[A\n",
            "Iteration:   3% 31/947 [00:19<09:22,  1.63it/s]\u001b[A\n",
            "Iteration:   3% 32/947 [00:20<09:21,  1.63it/s]\u001b[A\n",
            "Iteration:   3% 33/947 [00:21<09:22,  1.62it/s]\u001b[A\n",
            "Iteration:   4% 34/947 [00:21<09:21,  1.63it/s]\u001b[A\n",
            "Iteration:   4% 35/947 [00:22<09:22,  1.62it/s]\u001b[A\n",
            "Iteration:   4% 36/947 [00:22<09:22,  1.62it/s]\u001b[A\n",
            "Iteration:   4% 37/947 [00:23<09:21,  1.62it/s]\u001b[A\n",
            "Iteration:   4% 38/947 [00:24<09:20,  1.62it/s]\u001b[A\n",
            "Iteration:   4% 39/947 [00:24<09:19,  1.62it/s]\u001b[A\n",
            "Iteration:   4% 40/947 [00:25<09:20,  1.62it/s]\u001b[A\n",
            "Iteration:   4% 41/947 [00:25<09:18,  1.62it/s]\u001b[A\n",
            "Iteration:   4% 42/947 [00:26<09:19,  1.62it/s]\u001b[A\n",
            "Iteration:   5% 43/947 [00:27<09:19,  1.62it/s]\u001b[A\n",
            "Iteration:   5% 44/947 [00:27<09:17,  1.62it/s]\u001b[A\n",
            "Iteration:   5% 45/947 [00:28<09:16,  1.62it/s]\u001b[A\n",
            "Iteration:   5% 46/947 [00:29<09:16,  1.62it/s]\u001b[A\n",
            "Iteration:   5% 47/947 [00:29<09:15,  1.62it/s]\u001b[A\n",
            "Iteration:   5% 48/947 [00:30<09:16,  1.62it/s]\u001b[A\n",
            "Iteration:   5% 49/947 [00:30<09:14,  1.62it/s]\u001b[A\n",
            "Iteration:   5% 50/947 [00:31<09:14,  1.62it/s]\u001b[A\n",
            "Iteration:   5% 51/947 [00:32<09:13,  1.62it/s]\u001b[A\n",
            "Iteration:   5% 52/947 [00:32<09:15,  1.61it/s]\u001b[A\n",
            "Iteration:   6% 53/947 [00:33<09:12,  1.62it/s]\u001b[A\n",
            "Iteration:   6% 54/947 [00:34<09:13,  1.61it/s]\u001b[A\n",
            "Iteration:   6% 55/947 [00:34<09:11,  1.62it/s]\u001b[A\n",
            "Iteration:   6% 56/947 [00:35<09:12,  1.61it/s]\u001b[A\n",
            "Iteration:   6% 57/947 [00:35<09:11,  1.61it/s]\u001b[A\n",
            "Iteration:   6% 58/947 [00:36<09:11,  1.61it/s]\u001b[A\n",
            "Iteration:   6% 59/947 [00:37<09:11,  1.61it/s]\u001b[A\n",
            "Iteration:   6% 60/947 [00:37<09:08,  1.62it/s]\u001b[A\n",
            "Iteration:   6% 61/947 [00:38<09:11,  1.61it/s]\u001b[A\n",
            "Iteration:   7% 62/947 [00:38<09:07,  1.62it/s]\u001b[A\n",
            "Iteration:   7% 63/947 [00:39<09:08,  1.61it/s]\u001b[A\n",
            "Iteration:   7% 64/947 [00:40<09:07,  1.61it/s]\u001b[A\n",
            "Iteration:   7% 65/947 [00:40<09:07,  1.61it/s]\u001b[A\n",
            "Iteration:   7% 66/947 [00:41<09:06,  1.61it/s]\u001b[A\n",
            "Iteration:   7% 67/947 [00:42<09:05,  1.61it/s]\u001b[A\n",
            "Iteration:   7% 68/947 [00:42<09:05,  1.61it/s]\u001b[A\n",
            "Iteration:   7% 69/947 [00:43<09:05,  1.61it/s]\u001b[A\n",
            "Iteration:   7% 70/947 [00:43<09:06,  1.61it/s]\u001b[A\n",
            "Iteration:   7% 71/947 [00:44<09:04,  1.61it/s]\u001b[A\n",
            "Iteration:   8% 72/947 [00:45<09:04,  1.61it/s]\u001b[A\n",
            "Iteration:   8% 73/947 [00:45<09:04,  1.61it/s]\u001b[A\n",
            "Iteration:   8% 74/947 [00:46<09:03,  1.61it/s]\u001b[A\n",
            "Iteration:   8% 75/947 [00:47<09:02,  1.61it/s]\u001b[A\n",
            "Iteration:   8% 76/947 [00:47<09:03,  1.60it/s]\u001b[A\n",
            "Iteration:   8% 77/947 [00:48<09:03,  1.60it/s]\u001b[A\n",
            "Iteration:   8% 78/947 [00:48<09:03,  1.60it/s]\u001b[A\n",
            "Iteration:   8% 79/947 [00:49<09:03,  1.60it/s]\u001b[A\n",
            "Iteration:   8% 80/947 [00:50<09:02,  1.60it/s]\u001b[A\n",
            "Iteration:   9% 81/947 [00:50<09:01,  1.60it/s]\u001b[A\n",
            "Iteration:   9% 82/947 [00:51<09:02,  1.59it/s]\u001b[A\n",
            "Iteration:   9% 83/947 [00:52<09:00,  1.60it/s]\u001b[A\n",
            "Iteration:   9% 84/947 [00:52<08:59,  1.60it/s]\u001b[A\n",
            "Iteration:   9% 85/947 [00:53<08:58,  1.60it/s]\u001b[A\n",
            "Iteration:   9% 86/947 [00:53<08:59,  1.60it/s]\u001b[A\n",
            "Iteration:   9% 87/947 [00:54<09:00,  1.59it/s]\u001b[A\n",
            "Iteration:   9% 88/947 [00:55<08:57,  1.60it/s]\u001b[A\n",
            "Iteration:   9% 89/947 [00:55<08:56,  1.60it/s]\u001b[A\n",
            "Iteration:  10% 90/947 [00:56<08:57,  1.59it/s]\u001b[A\n",
            "Iteration:  10% 91/947 [00:57<08:57,  1.59it/s]\u001b[A\n",
            "Iteration:  10% 92/947 [00:57<08:56,  1.59it/s]\u001b[A\n",
            "Iteration:  10% 93/947 [00:58<08:56,  1.59it/s]\u001b[A\n",
            "Iteration:  10% 94/947 [00:58<08:56,  1.59it/s]\u001b[A\n",
            "Iteration:  10% 95/947 [00:59<08:54,  1.59it/s]\u001b[A\n",
            "Iteration:  10% 96/947 [01:00<08:54,  1.59it/s]\u001b[A\n",
            "Iteration:  10% 97/947 [01:00<08:53,  1.59it/s]\u001b[A\n",
            "Iteration:  10% 98/947 [01:01<08:53,  1.59it/s]\u001b[A\n",
            "Iteration:  10% 99/947 [01:02<08:52,  1.59it/s]\u001b[A\n",
            "Iteration:  11% 100/947 [01:02<08:52,  1.59it/s]\u001b[A\n",
            "Iteration:  11% 101/947 [01:03<08:51,  1.59it/s]\u001b[A\n",
            "Iteration:  11% 102/947 [01:03<08:51,  1.59it/s]\u001b[A\n",
            "Iteration:  11% 103/947 [01:04<08:50,  1.59it/s]\u001b[A\n",
            "Iteration:  11% 104/947 [01:05<08:50,  1.59it/s]\u001b[A\n",
            "Iteration:  11% 105/947 [01:05<08:48,  1.59it/s]\u001b[A\n",
            "Iteration:  11% 106/947 [01:06<08:48,  1.59it/s]\u001b[A\n",
            "Iteration:  11% 107/947 [01:07<08:47,  1.59it/s]\u001b[A\n",
            "Iteration:  11% 108/947 [01:07<08:49,  1.58it/s]\u001b[A\n",
            "Iteration:  12% 109/947 [01:08<08:47,  1.59it/s]\u001b[A\n",
            "Iteration:  12% 110/947 [01:09<08:46,  1.59it/s]\u001b[A\n",
            "Iteration:  12% 111/947 [01:09<08:46,  1.59it/s]\u001b[A\n",
            "Iteration:  12% 112/947 [01:10<08:46,  1.59it/s]\u001b[A\n",
            "Iteration:  12% 113/947 [01:10<08:46,  1.58it/s]\u001b[A\n",
            "Iteration:  12% 114/947 [01:11<08:43,  1.59it/s]\u001b[A\n",
            "Iteration:  12% 115/947 [01:12<08:43,  1.59it/s]\u001b[A\n",
            "Iteration:  12% 116/947 [01:12<08:42,  1.59it/s]\u001b[A\n",
            "Iteration:  12% 117/947 [01:13<08:42,  1.59it/s]\u001b[A\n",
            "Iteration:  12% 118/947 [01:14<08:42,  1.59it/s]\u001b[A\n",
            "Iteration:  13% 119/947 [01:14<08:43,  1.58it/s]\u001b[A\n",
            "Iteration:  13% 120/947 [01:15<08:42,  1.58it/s]\u001b[A\n",
            "Iteration:  13% 121/947 [01:15<08:42,  1.58it/s]\u001b[A\n",
            "Iteration:  13% 122/947 [01:16<08:41,  1.58it/s]\u001b[A\n",
            "Iteration:  13% 123/947 [01:17<08:42,  1.58it/s]\u001b[A\n",
            "Iteration:  13% 124/947 [01:17<08:41,  1.58it/s]\u001b[A\n",
            "Iteration:  13% 125/947 [01:18<08:41,  1.58it/s]\u001b[A\n",
            "Iteration:  13% 126/947 [01:19<08:41,  1.58it/s]\u001b[A\n",
            "Iteration:  13% 127/947 [01:19<08:39,  1.58it/s]\u001b[A\n",
            "Iteration:  14% 128/947 [01:20<08:38,  1.58it/s]\u001b[A\n",
            "Iteration:  14% 129/947 [01:21<08:38,  1.58it/s]\u001b[A\n",
            "Iteration:  14% 130/947 [01:21<08:38,  1.58it/s]\u001b[A\n",
            "Iteration:  14% 131/947 [01:22<08:38,  1.57it/s]\u001b[A\n",
            "Iteration:  14% 132/947 [01:22<08:41,  1.56it/s]\u001b[A\n",
            "Iteration:  14% 133/947 [01:23<08:37,  1.57it/s]\u001b[A\n",
            "Iteration:  14% 134/947 [01:24<08:37,  1.57it/s]\u001b[A\n",
            "Iteration:  14% 135/947 [01:24<08:35,  1.57it/s]\u001b[A\n",
            "Iteration:  14% 136/947 [01:25<08:35,  1.57it/s]\u001b[A\n",
            "Iteration:  14% 137/947 [01:26<08:35,  1.57it/s]\u001b[A\n",
            "Iteration:  15% 138/947 [01:26<08:34,  1.57it/s]\u001b[A\n",
            "Iteration:  15% 139/947 [01:27<08:35,  1.57it/s]\u001b[A\n",
            "Iteration:  15% 140/947 [01:28<08:36,  1.56it/s]\u001b[A\n",
            "Iteration:  15% 141/947 [01:28<08:34,  1.57it/s]\u001b[A\n",
            "Iteration:  15% 142/947 [01:29<08:35,  1.56it/s]\u001b[A\n",
            "Iteration:  15% 143/947 [01:29<08:34,  1.56it/s]\u001b[A\n",
            "Iteration:  15% 144/947 [01:30<08:34,  1.56it/s]\u001b[A\n",
            "Iteration:  15% 145/947 [01:31<08:33,  1.56it/s]\u001b[A\n",
            "Iteration:  15% 146/947 [01:31<08:33,  1.56it/s]\u001b[A\n",
            "Iteration:  16% 147/947 [01:32<08:32,  1.56it/s]\u001b[A\n",
            "Iteration:  16% 148/947 [01:33<08:32,  1.56it/s]\u001b[A\n",
            "Iteration:  16% 149/947 [01:33<08:30,  1.56it/s]\u001b[A\n",
            "Iteration:  16% 150/947 [01:34<08:31,  1.56it/s]\u001b[A\n",
            "Iteration:  16% 151/947 [01:35<08:30,  1.56it/s]\u001b[A\n",
            "Iteration:  16% 152/947 [01:35<08:29,  1.56it/s]\u001b[A\n",
            "Iteration:  16% 153/947 [01:36<08:29,  1.56it/s]\u001b[A\n",
            "Iteration:  16% 154/947 [01:37<08:28,  1.56it/s]\u001b[A\n",
            "Iteration:  16% 155/947 [01:37<08:28,  1.56it/s]\u001b[A\n",
            "Iteration:  16% 156/947 [01:38<08:28,  1.55it/s]\u001b[A\n",
            "Iteration:  17% 157/947 [01:38<08:28,  1.55it/s]\u001b[A\n",
            "Iteration:  17% 158/947 [01:39<08:27,  1.56it/s]\u001b[A\n",
            "Iteration:  17% 159/947 [01:40<08:26,  1.55it/s]\u001b[A\n",
            "Iteration:  17% 160/947 [01:40<08:26,  1.55it/s]\u001b[A\n",
            "Iteration:  17% 161/947 [01:41<08:26,  1.55it/s]\u001b[A\n",
            "Iteration:  17% 162/947 [01:42<08:27,  1.55it/s]\u001b[A\n",
            "Iteration:  17% 163/947 [01:42<08:25,  1.55it/s]\u001b[A\n",
            "Iteration:  17% 164/947 [01:43<08:25,  1.55it/s]\u001b[A\n",
            "Iteration:  17% 165/947 [01:44<08:24,  1.55it/s]\u001b[A\n",
            "Iteration:  18% 166/947 [01:44<08:24,  1.55it/s]\u001b[A\n",
            "Iteration:  18% 167/947 [01:45<08:25,  1.54it/s]\u001b[A\n",
            "Iteration:  18% 168/947 [01:46<08:23,  1.55it/s]\u001b[A\n",
            "Iteration:  18% 169/947 [01:46<08:23,  1.55it/s]\u001b[A\n",
            "Iteration:  18% 170/947 [01:47<08:22,  1.55it/s]\u001b[A\n",
            "Iteration:  18% 171/947 [01:48<08:22,  1.54it/s]\u001b[A\n",
            "Iteration:  18% 172/947 [01:48<08:21,  1.55it/s]\u001b[A\n",
            "Iteration:  18% 173/947 [01:49<08:22,  1.54it/s]\u001b[A\n",
            "Iteration:  18% 174/947 [01:49<08:20,  1.55it/s]\u001b[A\n",
            "Iteration:  18% 175/947 [01:50<08:19,  1.55it/s]\u001b[A\n",
            "Iteration:  19% 176/947 [01:51<08:19,  1.54it/s]\u001b[A\n",
            "Iteration:  19% 177/947 [01:51<08:20,  1.54it/s]\u001b[A\n",
            "Iteration:  19% 178/947 [01:52<08:17,  1.54it/s]\u001b[A\n",
            "Iteration:  19% 179/947 [01:53<08:17,  1.54it/s]\u001b[A\n",
            "Iteration:  19% 180/947 [01:53<08:16,  1.54it/s]\u001b[A\n",
            "Iteration:  19% 181/947 [01:54<08:16,  1.54it/s]\u001b[A\n",
            "Iteration:  19% 182/947 [01:55<08:15,  1.54it/s]\u001b[A\n",
            "Iteration:  19% 183/947 [01:55<08:15,  1.54it/s]\u001b[A\n",
            "Iteration:  19% 184/947 [01:56<08:15,  1.54it/s]\u001b[A\n",
            "Iteration:  20% 185/947 [01:57<08:13,  1.54it/s]\u001b[A\n",
            "Iteration:  20% 186/947 [01:57<08:13,  1.54it/s]\u001b[A\n",
            "Iteration:  20% 187/947 [01:58<08:14,  1.54it/s]\u001b[A\n",
            "Iteration:  20% 188/947 [01:59<08:12,  1.54it/s]\u001b[A\n",
            "Iteration:  20% 189/947 [01:59<08:11,  1.54it/s]\u001b[A\n",
            "Iteration:  20% 190/947 [02:00<08:11,  1.54it/s]\u001b[A\n",
            "Iteration:  20% 191/947 [02:00<08:11,  1.54it/s]\u001b[A\n",
            "Iteration:  20% 192/947 [02:01<08:10,  1.54it/s]\u001b[A\n",
            "Iteration:  20% 193/947 [02:02<08:09,  1.54it/s]\u001b[A\n",
            "Iteration:  20% 194/947 [02:02<08:08,  1.54it/s]\u001b[A\n",
            "Iteration:  21% 195/947 [02:03<08:07,  1.54it/s]\u001b[A\n",
            "Iteration:  21% 196/947 [02:04<08:06,  1.54it/s]\u001b[A\n",
            "Iteration:  21% 197/947 [02:04<08:06,  1.54it/s]\u001b[A\n",
            "Iteration:  21% 198/947 [02:05<08:06,  1.54it/s]\u001b[A\n",
            "Iteration:  21% 199/947 [02:06<08:04,  1.54it/s]\u001b[A\n",
            "Iteration:  21% 200/947 [02:06<08:03,  1.54it/s]\u001b[A\n",
            "Iteration:  21% 201/947 [02:07<08:03,  1.54it/s]\u001b[A\n",
            "Iteration:  21% 202/947 [02:08<08:02,  1.54it/s]\u001b[A\n",
            "Iteration:  21% 203/947 [02:08<08:01,  1.55it/s]\u001b[A\n",
            "Iteration:  22% 204/947 [02:09<08:00,  1.55it/s]\u001b[A\n",
            "Iteration:  22% 205/947 [02:10<07:59,  1.55it/s]\u001b[A\n",
            "Iteration:  22% 206/947 [02:10<07:57,  1.55it/s]\u001b[A\n",
            "Iteration:  22% 207/947 [02:11<07:57,  1.55it/s]\u001b[A\n",
            "Iteration:  22% 208/947 [02:11<07:56,  1.55it/s]\u001b[A\n",
            "Iteration:  22% 209/947 [02:12<07:57,  1.55it/s]\u001b[A\n",
            "Iteration:  22% 210/947 [02:13<07:55,  1.55it/s]\u001b[A\n",
            "Iteration:  22% 211/947 [02:13<07:52,  1.56it/s]\u001b[A\n",
            "Iteration:  22% 212/947 [02:14<07:52,  1.56it/s]\u001b[A\n",
            "Iteration:  22% 213/947 [02:15<07:52,  1.55it/s]\u001b[A\n",
            "Iteration:  23% 214/947 [02:15<07:50,  1.56it/s]\u001b[A\n",
            "Iteration:  23% 215/947 [02:16<07:54,  1.54it/s]\u001b[A\n",
            "Iteration:  23% 216/947 [02:17<07:48,  1.56it/s]\u001b[A\n",
            "Iteration:  23% 217/947 [02:17<07:49,  1.56it/s]\u001b[A\n",
            "Iteration:  23% 218/947 [02:18<07:47,  1.56it/s]\u001b[A\n",
            "Iteration:  23% 219/947 [02:19<07:47,  1.56it/s]\u001b[A\n",
            "Iteration:  23% 220/947 [02:19<07:47,  1.56it/s]\u001b[A\n",
            "Iteration:  23% 221/947 [02:20<07:46,  1.56it/s]\u001b[A\n",
            "Iteration:  23% 222/947 [02:20<07:45,  1.56it/s]\u001b[A\n",
            "Iteration:  24% 223/947 [02:21<07:44,  1.56it/s]\u001b[A\n",
            "Iteration:  24% 224/947 [02:22<07:42,  1.56it/s]\u001b[A\n",
            "Iteration:  24% 225/947 [02:22<07:42,  1.56it/s]\u001b[A\n",
            "Iteration:  24% 226/947 [02:23<07:41,  1.56it/s]\u001b[A\n",
            "Iteration:  24% 227/947 [02:24<07:41,  1.56it/s]\u001b[A\n",
            "Iteration:  24% 228/947 [02:24<07:40,  1.56it/s]\u001b[A\n",
            "Iteration:  24% 229/947 [02:25<07:39,  1.56it/s]\u001b[A\n",
            "Iteration:  24% 230/947 [02:26<07:38,  1.56it/s]\u001b[A\n",
            "Iteration:  24% 231/947 [02:26<07:37,  1.56it/s]\u001b[A\n",
            "Iteration:  24% 232/947 [02:27<07:37,  1.56it/s]\u001b[A\n",
            "Iteration:  25% 233/947 [02:28<07:36,  1.56it/s]\u001b[A\n",
            "Iteration:  25% 234/947 [02:28<07:35,  1.56it/s]\u001b[A\n",
            "Iteration:  25% 235/947 [02:29<07:34,  1.57it/s]\u001b[A\n",
            "Iteration:  25% 236/947 [02:29<07:33,  1.57it/s]\u001b[A\n",
            "Iteration:  25% 237/947 [02:30<07:33,  1.57it/s]\u001b[A\n",
            "Iteration:  25% 238/947 [02:31<07:33,  1.56it/s]\u001b[A\n",
            "Iteration:  25% 239/947 [02:31<07:33,  1.56it/s]\u001b[A\n",
            "Iteration:  25% 240/947 [02:32<07:31,  1.57it/s]\u001b[A\n",
            "Iteration:  25% 241/947 [02:33<07:32,  1.56it/s]\u001b[A\n",
            "Iteration:  26% 242/947 [02:33<07:30,  1.57it/s]\u001b[A\n",
            "Iteration:  26% 243/947 [02:34<07:29,  1.57it/s]\u001b[A\n",
            "Iteration:  26% 244/947 [02:35<07:28,  1.57it/s]\u001b[A\n",
            "Iteration:  26% 245/947 [02:35<07:28,  1.57it/s]\u001b[A\n",
            "Iteration:  26% 246/947 [02:36<07:27,  1.57it/s]\u001b[A\n",
            "Iteration:  26% 247/947 [02:36<07:28,  1.56it/s]\u001b[A\n",
            "Iteration:  26% 248/947 [02:37<07:27,  1.56it/s]\u001b[A\n",
            "Iteration:  26% 249/947 [02:38<07:27,  1.56it/s]\u001b[A\n",
            "Iteration:  26% 250/947 [02:38<07:25,  1.56it/s]\u001b[A\n",
            "Iteration:  27% 251/947 [02:39<07:25,  1.56it/s]\u001b[A\n",
            "Iteration:  27% 252/947 [02:40<07:24,  1.57it/s]\u001b[A\n",
            "Iteration:  27% 253/947 [02:40<07:24,  1.56it/s]\u001b[A\n",
            "Iteration:  27% 254/947 [02:41<07:22,  1.57it/s]\u001b[A\n",
            "Iteration:  27% 255/947 [02:42<07:22,  1.56it/s]\u001b[A\n",
            "Iteration:  27% 256/947 [02:42<07:20,  1.57it/s]\u001b[A\n",
            "Iteration:  27% 257/947 [02:43<07:20,  1.57it/s]\u001b[A\n",
            "Iteration:  27% 258/947 [02:43<07:19,  1.57it/s]\u001b[A\n",
            "Iteration:  27% 259/947 [02:44<07:19,  1.57it/s]\u001b[A\n",
            "Iteration:  27% 260/947 [02:45<07:18,  1.57it/s]\u001b[A\n",
            "Iteration:  28% 261/947 [02:45<07:18,  1.56it/s]\u001b[A\n",
            "Iteration:  28% 262/947 [02:46<07:17,  1.57it/s]\u001b[A\n",
            "Iteration:  28% 263/947 [02:47<07:17,  1.56it/s]\u001b[A\n",
            "Iteration:  28% 264/947 [02:47<07:15,  1.57it/s]\u001b[A\n",
            "Iteration:  28% 265/947 [02:48<07:16,  1.56it/s]\u001b[A\n",
            "Iteration:  28% 266/947 [02:49<07:18,  1.55it/s]\u001b[A\n",
            "Iteration:  28% 267/947 [02:49<07:14,  1.56it/s]\u001b[A\n",
            "Iteration:  28% 268/947 [02:50<07:14,  1.56it/s]\u001b[A\n",
            "Iteration:  28% 269/947 [02:51<07:13,  1.56it/s]\u001b[A\n",
            "Iteration:  29% 270/947 [02:51<07:12,  1.56it/s]\u001b[A\n",
            "Iteration:  29% 271/947 [02:52<07:12,  1.56it/s]\u001b[A\n",
            "Iteration:  29% 272/947 [02:52<07:13,  1.56it/s]\u001b[A\n",
            "Iteration:  29% 273/947 [02:53<07:12,  1.56it/s]\u001b[A\n",
            "Iteration:  29% 274/947 [02:54<07:11,  1.56it/s]\u001b[A\n",
            "Iteration:  29% 275/947 [02:54<07:11,  1.56it/s]\u001b[A\n",
            "Iteration:  29% 276/947 [02:55<07:10,  1.56it/s]\u001b[A\n",
            "Iteration:  29% 277/947 [02:56<07:09,  1.56it/s]\u001b[A\n",
            "Iteration:  29% 278/947 [02:56<07:09,  1.56it/s]\u001b[A\n",
            "Iteration:  29% 279/947 [02:57<07:08,  1.56it/s]\u001b[A\n",
            "Iteration:  30% 280/947 [02:58<07:07,  1.56it/s]\u001b[A\n",
            "Iteration:  30% 281/947 [02:58<07:07,  1.56it/s]\u001b[A\n",
            "Iteration:  30% 282/947 [02:59<07:06,  1.56it/s]\u001b[A\n",
            "Iteration:  30% 283/947 [02:59<07:05,  1.56it/s]\u001b[A\n",
            "Iteration:  30% 284/947 [03:00<07:05,  1.56it/s]\u001b[A\n",
            "Iteration:  30% 285/947 [03:01<07:05,  1.56it/s]\u001b[A\n",
            "Iteration:  30% 286/947 [03:01<07:05,  1.55it/s]\u001b[A\n",
            "Iteration:  30% 287/947 [03:02<07:04,  1.56it/s]\u001b[A\n",
            "Iteration:  30% 288/947 [03:03<07:04,  1.55it/s]\u001b[A\n",
            "Iteration:  31% 289/947 [03:03<07:02,  1.56it/s]\u001b[A\n",
            "Iteration:  31% 290/947 [03:04<07:03,  1.55it/s]\u001b[A\n",
            "Iteration:  31% 291/947 [03:05<07:01,  1.55it/s]\u001b[A\n",
            "Iteration:  31% 292/947 [03:05<07:01,  1.55it/s]\u001b[A\n",
            "Iteration:  31% 293/947 [03:06<07:00,  1.55it/s]\u001b[A\n",
            "Iteration:  31% 294/947 [03:07<06:59,  1.56it/s]\u001b[A\n",
            "Iteration:  31% 295/947 [03:07<06:59,  1.56it/s]\u001b[A\n",
            "Iteration:  31% 296/947 [03:08<06:59,  1.55it/s]\u001b[A\n",
            "Iteration:  31% 297/947 [03:09<06:58,  1.55it/s]\u001b[A\n",
            "Iteration:  31% 298/947 [03:09<06:58,  1.55it/s]\u001b[A\n",
            "Iteration:  32% 299/947 [03:10<06:56,  1.55it/s]\u001b[A\n",
            "Iteration:  32% 300/947 [03:10<06:56,  1.55it/s]\u001b[A\n",
            "Iteration:  32% 301/947 [03:11<06:55,  1.56it/s]\u001b[A\n",
            "Iteration:  32% 302/947 [03:12<06:54,  1.56it/s]\u001b[A\n",
            "Iteration:  32% 303/947 [03:12<06:54,  1.55it/s]\u001b[A\n",
            "Iteration:  32% 304/947 [03:13<06:53,  1.55it/s]\u001b[A\n",
            "Iteration:  32% 305/947 [03:14<06:53,  1.55it/s]\u001b[A\n",
            "Iteration:  32% 306/947 [03:14<06:52,  1.55it/s]\u001b[A\n",
            "Iteration:  32% 307/947 [03:15<06:52,  1.55it/s]\u001b[A\n",
            "Iteration:  33% 308/947 [03:16<06:51,  1.55it/s]\u001b[A\n",
            "Iteration:  33% 309/947 [03:16<06:50,  1.55it/s]\u001b[A\n",
            "Iteration:  33% 310/947 [03:17<06:49,  1.56it/s]\u001b[A\n",
            "Iteration:  33% 311/947 [03:18<06:49,  1.55it/s]\u001b[A\n",
            "Iteration:  33% 312/947 [03:18<06:47,  1.56it/s]\u001b[A\n",
            "Iteration:  33% 313/947 [03:19<06:47,  1.55it/s]\u001b[A\n",
            "Iteration:  33% 314/947 [03:19<06:47,  1.55it/s]\u001b[A\n",
            "Iteration:  33% 315/947 [03:20<06:46,  1.55it/s]\u001b[A\n",
            "Iteration:  33% 316/947 [03:21<06:45,  1.56it/s]\u001b[A\n",
            "Iteration:  33% 317/947 [03:21<06:45,  1.55it/s]\u001b[A\n",
            "Iteration:  34% 318/947 [03:22<06:44,  1.55it/s]\u001b[A\n",
            "Iteration:  34% 319/947 [03:23<06:43,  1.56it/s]\u001b[A\n",
            "Iteration:  34% 320/947 [03:23<06:42,  1.56it/s]\u001b[A\n",
            "Iteration:  34% 321/947 [03:24<06:42,  1.55it/s]\u001b[A\n",
            "Iteration:  34% 322/947 [03:25<06:41,  1.56it/s]\u001b[A\n",
            "Iteration:  34% 323/947 [03:25<06:40,  1.56it/s]\u001b[A\n",
            "Iteration:  34% 324/947 [03:26<06:40,  1.56it/s]\u001b[A\n",
            "Iteration:  34% 325/947 [03:27<06:39,  1.56it/s]\u001b[A\n",
            "Iteration:  34% 326/947 [03:27<06:38,  1.56it/s]\u001b[A\n",
            "Iteration:  35% 327/947 [03:28<06:38,  1.56it/s]\u001b[A\n",
            "Iteration:  35% 328/947 [03:28<06:37,  1.56it/s]\u001b[A\n",
            "Iteration:  35% 329/947 [03:29<06:36,  1.56it/s]\u001b[A\n",
            "Iteration:  35% 330/947 [03:30<06:35,  1.56it/s]\u001b[A\n",
            "Iteration:  35% 331/947 [03:30<06:35,  1.56it/s]\u001b[A\n",
            "Iteration:  35% 332/947 [03:31<06:35,  1.56it/s]\u001b[A\n",
            "Iteration:  35% 333/947 [03:32<06:34,  1.56it/s]\u001b[A\n",
            "Iteration:  35% 334/947 [03:32<06:33,  1.56it/s]\u001b[A\n",
            "Iteration:  35% 335/947 [03:33<06:32,  1.56it/s]\u001b[A\n",
            "Iteration:  35% 336/947 [03:34<06:32,  1.56it/s]\u001b[A\n",
            "Iteration:  36% 337/947 [03:34<06:31,  1.56it/s]\u001b[A\n",
            "Iteration:  36% 338/947 [03:35<06:30,  1.56it/s]\u001b[A\n",
            "Iteration:  36% 339/947 [03:35<06:29,  1.56it/s]\u001b[A\n",
            "Iteration:  36% 340/947 [03:36<06:29,  1.56it/s]\u001b[A\n",
            "Iteration:  36% 341/947 [03:37<06:28,  1.56it/s]\u001b[A\n",
            "Iteration:  36% 342/947 [03:37<06:27,  1.56it/s]\u001b[A\n",
            "Iteration:  36% 343/947 [03:38<06:27,  1.56it/s]\u001b[A\n",
            "Iteration:  36% 344/947 [03:39<06:26,  1.56it/s]\u001b[A\n",
            "Iteration:  36% 345/947 [03:39<06:25,  1.56it/s]\u001b[A\n",
            "Iteration:  37% 346/947 [03:40<06:25,  1.56it/s]\u001b[A\n",
            "Iteration:  37% 347/947 [03:41<06:25,  1.56it/s]\u001b[A\n",
            "Iteration:  37% 348/947 [03:41<06:24,  1.56it/s]\u001b[A\n",
            "Iteration:  37% 349/947 [03:42<06:24,  1.56it/s]\u001b[A\n",
            "Iteration:  37% 350/947 [03:43<06:23,  1.56it/s]\u001b[A\n",
            "Iteration:  37% 351/947 [03:43<06:23,  1.55it/s]\u001b[A\n",
            "Iteration:  37% 352/947 [03:44<06:22,  1.56it/s]\u001b[A\n",
            "Iteration:  37% 353/947 [03:44<06:21,  1.56it/s]\u001b[A\n",
            "Iteration:  37% 354/947 [03:45<06:20,  1.56it/s]\u001b[A\n",
            "Iteration:  37% 355/947 [03:46<06:20,  1.56it/s]\u001b[A\n",
            "Iteration:  38% 356/947 [03:46<06:19,  1.56it/s]\u001b[A\n",
            "Iteration:  38% 357/947 [03:47<06:18,  1.56it/s]\u001b[A\n",
            "Iteration:  38% 358/947 [03:48<06:18,  1.56it/s]\u001b[A\n",
            "Iteration:  38% 359/947 [03:48<06:18,  1.56it/s]\u001b[A\n",
            "Iteration:  38% 360/947 [03:49<06:16,  1.56it/s]\u001b[A\n",
            "Iteration:  38% 361/947 [03:50<06:15,  1.56it/s]\u001b[A\n",
            "Iteration:  38% 362/947 [03:50<06:15,  1.56it/s]\u001b[A\n",
            "Iteration:  38% 363/947 [03:51<06:14,  1.56it/s]\u001b[A\n",
            "Iteration:  38% 364/947 [03:52<06:14,  1.56it/s]\u001b[A\n",
            "Iteration:  39% 365/947 [03:52<06:13,  1.56it/s]\u001b[A\n",
            "Iteration:  39% 366/947 [03:53<06:13,  1.56it/s]\u001b[A\n",
            "Iteration:  39% 367/947 [03:53<06:12,  1.56it/s]\u001b[A\n",
            "Iteration:  39% 368/947 [03:54<06:11,  1.56it/s]\u001b[A\n",
            "Iteration:  39% 369/947 [03:55<06:10,  1.56it/s]\u001b[A\n",
            "Iteration:  39% 370/947 [03:55<06:09,  1.56it/s]\u001b[A\n",
            "Iteration:  39% 371/947 [03:56<06:09,  1.56it/s]\u001b[A\n",
            "Iteration:  39% 372/947 [03:57<06:08,  1.56it/s]\u001b[A\n",
            "Iteration:  39% 373/947 [03:57<06:07,  1.56it/s]\u001b[A\n",
            "Iteration:  39% 374/947 [03:58<06:07,  1.56it/s]\u001b[A\n",
            "Iteration:  40% 375/947 [03:59<06:06,  1.56it/s]\u001b[A\n",
            "Iteration:  40% 376/947 [03:59<06:05,  1.56it/s]\u001b[A\n",
            "Iteration:  40% 377/947 [04:00<06:05,  1.56it/s]\u001b[A\n",
            "Iteration:  40% 378/947 [04:01<06:05,  1.56it/s]\u001b[A\n",
            "Iteration:  40% 379/947 [04:01<06:03,  1.56it/s]\u001b[A\n",
            "Iteration:  40% 380/947 [04:02<06:03,  1.56it/s]\u001b[A\n",
            "Iteration:  40% 381/947 [04:02<06:02,  1.56it/s]\u001b[A\n",
            "Iteration:  40% 382/947 [04:03<06:02,  1.56it/s]\u001b[A\n",
            "Iteration:  40% 383/947 [04:04<06:01,  1.56it/s]\u001b[A\n",
            "Iteration:  41% 384/947 [04:04<06:01,  1.56it/s]\u001b[A\n",
            "Iteration:  41% 385/947 [04:05<06:00,  1.56it/s]\u001b[A\n",
            "Iteration:  41% 386/947 [04:06<06:00,  1.56it/s]\u001b[A\n",
            "Iteration:  41% 387/947 [04:06<05:58,  1.56it/s]\u001b[A\n",
            "Iteration:  41% 388/947 [04:07<05:58,  1.56it/s]\u001b[A\n",
            "Iteration:  41% 389/947 [04:08<05:58,  1.56it/s]\u001b[A\n",
            "Iteration:  41% 390/947 [04:08<05:56,  1.56it/s]\u001b[A\n",
            "Iteration:  41% 391/947 [04:09<05:56,  1.56it/s]\u001b[A\n",
            "Iteration:  41% 392/947 [04:09<05:54,  1.56it/s]\u001b[A\n",
            "Iteration:  41% 393/947 [04:10<05:54,  1.56it/s]\u001b[A\n",
            "Iteration:  42% 394/947 [04:11<05:53,  1.56it/s]\u001b[A\n",
            "Iteration:  42% 395/947 [04:11<05:53,  1.56it/s]\u001b[A\n",
            "Iteration:  42% 396/947 [04:12<05:52,  1.56it/s]\u001b[A\n",
            "Iteration:  42% 397/947 [04:13<05:51,  1.56it/s]\u001b[A\n",
            "Iteration:  42% 398/947 [04:13<05:51,  1.56it/s]\u001b[A\n",
            "Iteration:  42% 399/947 [04:14<05:50,  1.56it/s]\u001b[A\n",
            "Iteration:  42% 400/947 [04:15<05:50,  1.56it/s]\u001b[A\n",
            "Iteration:  42% 401/947 [04:15<05:50,  1.56it/s]\u001b[A\n",
            "Iteration:  42% 402/947 [04:16<05:48,  1.56it/s]\u001b[A\n",
            "Iteration:  43% 403/947 [04:17<05:48,  1.56it/s]\u001b[A\n",
            "Iteration:  43% 404/947 [04:17<05:48,  1.56it/s]\u001b[A\n",
            "Iteration:  43% 405/947 [04:18<05:46,  1.56it/s]\u001b[A\n",
            "Iteration:  43% 406/947 [04:18<05:46,  1.56it/s]\u001b[A\n",
            "Iteration:  43% 407/947 [04:19<05:45,  1.56it/s]\u001b[A\n",
            "Iteration:  43% 408/947 [04:20<05:46,  1.55it/s]\u001b[A\n",
            "Iteration:  43% 409/947 [04:20<05:44,  1.56it/s]\u001b[A\n",
            "Iteration:  43% 410/947 [04:21<05:44,  1.56it/s]\u001b[A\n",
            "Iteration:  43% 411/947 [04:22<05:43,  1.56it/s]\u001b[A\n",
            "Iteration:  44% 412/947 [04:22<05:43,  1.56it/s]\u001b[A\n",
            "Iteration:  44% 413/947 [04:23<05:42,  1.56it/s]\u001b[A\n",
            "Iteration:  44% 414/947 [04:24<05:41,  1.56it/s]\u001b[A\n",
            "Iteration:  44% 415/947 [04:24<05:40,  1.56it/s]\u001b[A\n",
            "Iteration:  44% 416/947 [04:25<05:40,  1.56it/s]\u001b[A\n",
            "Iteration:  44% 417/947 [04:26<05:39,  1.56it/s]\u001b[A\n",
            "Iteration:  44% 418/947 [04:26<05:39,  1.56it/s]\u001b[A\n",
            "Iteration:  44% 419/947 [04:27<05:39,  1.56it/s]\u001b[A\n",
            "Iteration:  44% 420/947 [04:27<05:39,  1.55it/s]\u001b[A\n",
            "Iteration:  44% 421/947 [04:28<05:37,  1.56it/s]\u001b[A\n",
            "Iteration:  45% 422/947 [04:29<05:36,  1.56it/s]\u001b[A\n",
            "Iteration:  45% 423/947 [04:29<05:35,  1.56it/s]\u001b[A\n",
            "Iteration:  45% 424/947 [04:30<05:34,  1.56it/s]\u001b[A\n",
            "Iteration:  45% 425/947 [04:31<05:34,  1.56it/s]\u001b[A\n",
            "Iteration:  45% 426/947 [04:31<05:33,  1.56it/s]\u001b[A\n",
            "Iteration:  45% 427/947 [04:32<05:34,  1.56it/s]\u001b[A\n",
            "Iteration:  45% 428/947 [04:33<05:32,  1.56it/s]\u001b[A\n",
            "Iteration:  45% 429/947 [04:33<05:31,  1.56it/s]\u001b[A\n",
            "Iteration:  45% 430/947 [04:34<05:31,  1.56it/s]\u001b[A\n",
            "Iteration:  46% 431/947 [04:34<05:30,  1.56it/s]\u001b[A\n",
            "Iteration:  46% 432/947 [04:35<05:29,  1.56it/s]\u001b[A\n",
            "Iteration:  46% 433/947 [04:36<05:29,  1.56it/s]\u001b[A\n",
            "Iteration:  46% 434/947 [04:36<05:28,  1.56it/s]\u001b[A\n",
            "Iteration:  46% 435/947 [04:37<05:28,  1.56it/s]\u001b[A\n",
            "Iteration:  46% 436/947 [04:38<05:27,  1.56it/s]\u001b[A\n",
            "Iteration:  46% 437/947 [04:38<05:27,  1.56it/s]\u001b[A\n",
            "Iteration:  46% 438/947 [04:39<05:27,  1.56it/s]\u001b[A\n",
            "Iteration:  46% 439/947 [04:40<05:25,  1.56it/s]\u001b[A\n",
            "Iteration:  46% 440/947 [04:40<05:24,  1.56it/s]\u001b[A\n",
            "Iteration:  47% 441/947 [04:41<05:24,  1.56it/s]\u001b[A\n",
            "Iteration:  47% 442/947 [04:42<05:23,  1.56it/s]\u001b[A\n",
            "Iteration:  47% 443/947 [04:42<05:23,  1.56it/s]\u001b[A\n",
            "Iteration:  47% 444/947 [04:43<05:22,  1.56it/s]\u001b[A\n",
            "Iteration:  47% 445/947 [04:43<05:21,  1.56it/s]\u001b[A\n",
            "Iteration:  47% 446/947 [04:44<05:20,  1.56it/s]\u001b[A\n",
            "Iteration:  47% 447/947 [04:45<05:20,  1.56it/s]\u001b[A\n",
            "Iteration:  47% 448/947 [04:45<05:19,  1.56it/s]\u001b[A\n",
            "Iteration:  47% 449/947 [04:46<05:19,  1.56it/s]\u001b[A\n",
            "Iteration:  48% 450/947 [04:47<05:19,  1.56it/s]\u001b[A\n",
            "Iteration:  48% 451/947 [04:47<05:18,  1.56it/s]\u001b[A\n",
            "Iteration:  48% 452/947 [04:48<05:18,  1.56it/s]\u001b[A\n",
            "Iteration:  48% 453/947 [04:49<05:16,  1.56it/s]\u001b[A\n",
            "Iteration:  48% 454/947 [04:49<05:16,  1.56it/s]\u001b[A\n",
            "Iteration:  48% 455/947 [04:50<05:15,  1.56it/s]\u001b[A\n",
            "Iteration:  48% 456/947 [04:51<05:15,  1.56it/s]\u001b[A\n",
            "Iteration:  48% 457/947 [04:51<05:14,  1.56it/s]\u001b[A\n",
            "Iteration:  48% 458/947 [04:52<05:14,  1.56it/s]\u001b[A\n",
            "Iteration:  48% 459/947 [04:52<05:13,  1.56it/s]\u001b[A\n",
            "Iteration:  49% 460/947 [04:53<05:12,  1.56it/s]\u001b[A\n",
            "Iteration:  49% 461/947 [04:54<05:12,  1.56it/s]\u001b[A\n",
            "Iteration:  49% 462/947 [04:54<05:11,  1.56it/s]\u001b[A\n",
            "Iteration:  49% 463/947 [04:55<05:10,  1.56it/s]\u001b[A\n",
            "Iteration:  49% 464/947 [04:56<05:09,  1.56it/s]\u001b[A\n",
            "Iteration:  49% 465/947 [04:56<05:09,  1.56it/s]\u001b[A\n",
            "Iteration:  49% 466/947 [04:57<05:08,  1.56it/s]\u001b[A\n",
            "Iteration:  49% 467/947 [04:58<05:07,  1.56it/s]\u001b[A\n",
            "Iteration:  49% 468/947 [04:58<05:07,  1.56it/s]\u001b[A\n",
            "Iteration:  50% 469/947 [04:59<05:07,  1.56it/s]\u001b[A\n",
            "Iteration:  50% 470/947 [05:00<05:08,  1.54it/s]\u001b[A\n",
            "Iteration:  50% 471/947 [05:00<05:05,  1.56it/s]\u001b[A\n",
            "Iteration:  50% 472/947 [05:01<05:05,  1.56it/s]\u001b[A\n",
            "Iteration:  50% 473/947 [05:01<05:05,  1.55it/s]\u001b[A\n",
            "Iteration:  50% 474/947 [05:02<05:03,  1.56it/s]\u001b[A\n",
            "Iteration:  50% 475/947 [05:03<05:03,  1.55it/s]\u001b[A\n",
            "Iteration:  50% 476/947 [05:03<05:02,  1.56it/s]\u001b[A\n",
            "Iteration:  50% 477/947 [05:04<05:02,  1.56it/s]\u001b[A\n",
            "Iteration:  50% 478/947 [05:05<05:00,  1.56it/s]\u001b[A\n",
            "Iteration:  51% 479/947 [05:05<05:00,  1.56it/s]\u001b[A\n",
            "Iteration:  51% 480/947 [05:06<05:00,  1.56it/s]\u001b[A\n",
            "Iteration:  51% 481/947 [05:07<04:59,  1.56it/s]\u001b[A\n",
            "Iteration:  51% 482/947 [05:07<04:58,  1.56it/s]\u001b[A\n",
            "Iteration:  51% 483/947 [05:08<04:57,  1.56it/s]\u001b[A\n",
            "Iteration:  51% 484/947 [05:09<04:57,  1.56it/s]\u001b[A\n",
            "Iteration:  51% 485/947 [05:09<04:56,  1.56it/s]\u001b[A\n",
            "Iteration:  51% 486/947 [05:10<04:56,  1.56it/s]\u001b[A\n",
            "Iteration:  51% 487/947 [05:10<04:55,  1.56it/s]\u001b[A\n",
            "Iteration:  52% 488/947 [05:11<04:55,  1.55it/s]\u001b[A\n",
            "Iteration:  52% 489/947 [05:12<04:53,  1.56it/s]\u001b[A\n",
            "Iteration:  52% 490/947 [05:12<04:53,  1.56it/s]\u001b[A\n",
            "Iteration:  52% 491/947 [05:13<04:52,  1.56it/s]\u001b[A\n",
            "Iteration:  52% 492/947 [05:14<04:51,  1.56it/s]\u001b[A\n",
            "Iteration:  52% 493/947 [05:14<04:51,  1.56it/s]\u001b[A\n",
            "Iteration:  52% 494/947 [05:15<04:51,  1.56it/s]\u001b[A\n",
            "Iteration:  52% 495/947 [05:16<04:50,  1.56it/s]\u001b[A\n",
            "Iteration:  52% 496/947 [05:16<04:49,  1.56it/s]\u001b[A\n",
            "Iteration:  52% 497/947 [05:17<04:48,  1.56it/s]\u001b[A\n",
            "Iteration:  53% 498/947 [05:17<04:48,  1.56it/s]\u001b[A\n",
            "Iteration:  53% 499/947 [05:18<04:48,  1.56it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "12/19/2020 23:21:05 - INFO - transformers.configuration_utils -   Configuration saved in essays_model/checkpoint-500/config.json\n",
            "12/19/2020 23:21:07 - INFO - transformers.modeling_utils -   Model weights saved in essays_model/checkpoint-500/pytorch_model.bin\n",
            "12/19/2020 23:21:07 - INFO - __main__ -   Saving model checkpoint to essays_model/checkpoint-500\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
            "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n",
            "12/19/2020 23:21:15 - INFO - __main__ -   Saving optimizer and scheduler states to essays_model/checkpoint-500\n",
            "\n",
            "Iteration:  53% 500/947 [05:30<28:45,  3.86s/it]\u001b[A\n",
            "Iteration:  53% 501/947 [05:30<21:33,  2.90s/it]\u001b[A\n",
            "Iteration:  53% 502/947 [05:31<16:28,  2.22s/it]\u001b[A\n",
            "Iteration:  53% 503/947 [05:31<12:54,  1.75s/it]\u001b[A\n",
            "Iteration:  53% 504/947 [05:32<10:25,  1.41s/it]\u001b[A\n",
            "Iteration:  53% 505/947 [05:33<08:41,  1.18s/it]\u001b[A\n",
            "Iteration:  53% 506/947 [05:33<07:29,  1.02s/it]\u001b[A\n",
            "Iteration:  54% 507/947 [05:34<06:37,  1.11it/s]\u001b[A\n",
            "Iteration:  54% 508/947 [05:35<06:01,  1.21it/s]\u001b[A\n",
            "Iteration:  54% 509/947 [05:35<05:36,  1.30it/s]\u001b[A\n",
            "Iteration:  54% 510/947 [05:36<05:18,  1.37it/s]\u001b[A\n",
            "Iteration:  54% 511/947 [05:37<05:06,  1.42it/s]\u001b[A\n",
            "Iteration:  54% 512/947 [05:37<04:57,  1.46it/s]\u001b[A\n",
            "Iteration:  54% 513/947 [05:38<04:52,  1.49it/s]\u001b[A\n",
            "Iteration:  54% 514/947 [05:38<04:46,  1.51it/s]\u001b[A\n",
            "Iteration:  54% 515/947 [05:39<04:43,  1.52it/s]\u001b[A\n",
            "Iteration:  54% 516/947 [05:40<04:40,  1.53it/s]\u001b[A\n",
            "Iteration:  55% 517/947 [05:40<04:38,  1.54it/s]\u001b[A\n",
            "Iteration:  55% 518/947 [05:41<04:37,  1.55it/s]\u001b[A\n",
            "Iteration:  55% 519/947 [05:42<04:36,  1.55it/s]\u001b[A\n",
            "Iteration:  55% 520/947 [05:42<04:34,  1.55it/s]\u001b[A\n",
            "Iteration:  55% 521/947 [05:43<04:34,  1.55it/s]\u001b[A\n",
            "Iteration:  55% 522/947 [05:44<04:33,  1.56it/s]\u001b[A\n",
            "Iteration:  55% 523/947 [05:44<04:33,  1.55it/s]\u001b[A\n",
            "Iteration:  55% 524/947 [05:45<04:33,  1.55it/s]\u001b[A\n",
            "Iteration:  55% 525/947 [05:46<04:32,  1.55it/s]\u001b[A\n",
            "Iteration:  56% 526/947 [05:46<04:31,  1.55it/s]\u001b[A\n",
            "Iteration:  56% 527/947 [05:47<04:31,  1.55it/s]\u001b[A\n",
            "Iteration:  56% 528/947 [05:47<04:30,  1.55it/s]\u001b[A\n",
            "Iteration:  56% 529/947 [05:48<04:30,  1.55it/s]\u001b[A\n",
            "Iteration:  56% 530/947 [05:49<04:30,  1.54it/s]\u001b[A\n",
            "Iteration:  56% 531/947 [05:49<04:29,  1.54it/s]\u001b[A\n",
            "Iteration:  56% 532/947 [05:50<04:28,  1.54it/s]\u001b[A\n",
            "Iteration:  56% 533/947 [05:51<04:28,  1.54it/s]\u001b[A\n",
            "Iteration:  56% 534/947 [05:51<04:27,  1.54it/s]\u001b[A\n",
            "Iteration:  56% 535/947 [05:52<04:27,  1.54it/s]\u001b[A\n",
            "Iteration:  57% 536/947 [05:53<04:26,  1.54it/s]\u001b[A\n",
            "Iteration:  57% 537/947 [05:53<04:25,  1.54it/s]\u001b[A\n",
            "Iteration:  57% 538/947 [05:54<04:25,  1.54it/s]\u001b[A\n",
            "Iteration:  57% 539/947 [05:55<04:24,  1.54it/s]\u001b[A\n",
            "Iteration:  57% 540/947 [05:55<04:23,  1.54it/s]\u001b[A\n",
            "Iteration:  57% 541/947 [05:56<04:23,  1.54it/s]\u001b[A\n",
            "Iteration:  57% 542/947 [05:57<04:22,  1.54it/s]\u001b[A\n",
            "Iteration:  57% 543/947 [05:57<04:22,  1.54it/s]\u001b[A\n",
            "Iteration:  57% 544/947 [05:58<04:21,  1.54it/s]\u001b[A\n",
            "Iteration:  58% 545/947 [05:59<04:20,  1.54it/s]\u001b[A\n",
            "Iteration:  58% 546/947 [05:59<04:20,  1.54it/s]\u001b[A\n",
            "Iteration:  58% 547/947 [06:00<04:19,  1.54it/s]\u001b[A\n",
            "Iteration:  58% 548/947 [06:00<04:18,  1.54it/s]\u001b[A\n",
            "Iteration:  58% 549/947 [06:01<04:18,  1.54it/s]\u001b[A\n",
            "Iteration:  58% 550/947 [06:02<04:16,  1.55it/s]\u001b[A\n",
            "Iteration:  58% 551/947 [06:02<04:16,  1.55it/s]\u001b[A\n",
            "Iteration:  58% 552/947 [06:03<04:15,  1.55it/s]\u001b[A\n",
            "Iteration:  58% 553/947 [06:04<04:14,  1.55it/s]\u001b[A\n",
            "Iteration:  59% 554/947 [06:04<04:14,  1.54it/s]\u001b[A\n",
            "Iteration:  59% 555/947 [06:05<04:13,  1.54it/s]\u001b[A\n",
            "Iteration:  59% 556/947 [06:06<04:13,  1.54it/s]\u001b[A\n",
            "Iteration:  59% 557/947 [06:06<04:12,  1.54it/s]\u001b[A\n",
            "Iteration:  59% 558/947 [06:07<04:11,  1.54it/s]\u001b[A\n",
            "Iteration:  59% 559/947 [06:08<04:11,  1.54it/s]\u001b[A\n",
            "Iteration:  59% 560/947 [06:08<04:10,  1.54it/s]\u001b[A\n",
            "Iteration:  59% 561/947 [06:09<04:10,  1.54it/s]\u001b[A\n",
            "Iteration:  59% 562/947 [06:10<04:09,  1.55it/s]\u001b[A\n",
            "Iteration:  59% 563/947 [06:10<04:09,  1.54it/s]\u001b[A\n",
            "Iteration:  60% 564/947 [06:11<04:07,  1.55it/s]\u001b[A\n",
            "Iteration:  60% 565/947 [06:11<04:07,  1.54it/s]\u001b[A\n",
            "Iteration:  60% 566/947 [06:12<04:07,  1.54it/s]\u001b[A\n",
            "Iteration:  60% 567/947 [06:13<04:05,  1.55it/s]\u001b[A\n",
            "Iteration:  60% 568/947 [06:13<04:05,  1.54it/s]\u001b[A\n",
            "Iteration:  60% 569/947 [06:14<04:04,  1.55it/s]\u001b[A\n",
            "Iteration:  60% 570/947 [06:15<04:03,  1.55it/s]\u001b[A\n",
            "Iteration:  60% 571/947 [06:15<04:02,  1.55it/s]\u001b[A\n",
            "Iteration:  60% 572/947 [06:16<04:02,  1.55it/s]\u001b[A\n",
            "Iteration:  61% 573/947 [06:17<04:00,  1.55it/s]\u001b[A\n",
            "Iteration:  61% 574/947 [06:17<04:00,  1.55it/s]\u001b[A\n",
            "Iteration:  61% 575/947 [06:18<03:59,  1.55it/s]\u001b[A\n",
            "Iteration:  61% 576/947 [06:19<03:59,  1.55it/s]\u001b[A\n",
            "Iteration:  61% 577/947 [06:19<03:57,  1.56it/s]\u001b[A\n",
            "Iteration:  61% 578/947 [06:20<03:56,  1.56it/s]\u001b[A\n",
            "Iteration:  61% 579/947 [06:20<03:56,  1.56it/s]\u001b[A\n",
            "Iteration:  61% 580/947 [06:21<03:55,  1.56it/s]\u001b[A\n",
            "Iteration:  61% 581/947 [06:22<03:54,  1.56it/s]\u001b[A\n",
            "Iteration:  61% 582/947 [06:22<03:54,  1.56it/s]\u001b[A\n",
            "Iteration:  62% 583/947 [06:23<03:53,  1.56it/s]\u001b[A\n",
            "Iteration:  62% 584/947 [06:24<03:52,  1.56it/s]\u001b[A\n",
            "Iteration:  62% 585/947 [06:24<03:52,  1.56it/s]\u001b[A\n",
            "Iteration:  62% 586/947 [06:25<03:51,  1.56it/s]\u001b[A\n",
            "Iteration:  62% 587/947 [06:26<03:50,  1.56it/s]\u001b[A\n",
            "Iteration:  62% 588/947 [06:26<03:49,  1.57it/s]\u001b[A\n",
            "Iteration:  62% 589/947 [06:27<03:48,  1.56it/s]\u001b[A\n",
            "Iteration:  62% 590/947 [06:28<03:48,  1.56it/s]\u001b[A\n",
            "Iteration:  62% 591/947 [06:28<03:47,  1.56it/s]\u001b[A\n",
            "Iteration:  63% 592/947 [06:29<03:47,  1.56it/s]\u001b[A\n",
            "Iteration:  63% 593/947 [06:29<03:46,  1.56it/s]\u001b[A\n",
            "Iteration:  63% 594/947 [06:30<03:45,  1.56it/s]\u001b[A\n",
            "Iteration:  63% 595/947 [06:31<03:44,  1.57it/s]\u001b[A\n",
            "Iteration:  63% 596/947 [06:31<03:44,  1.56it/s]\u001b[A\n",
            "Iteration:  63% 597/947 [06:32<03:43,  1.57it/s]\u001b[A\n",
            "Iteration:  63% 598/947 [06:33<03:43,  1.56it/s]\u001b[A\n",
            "Iteration:  63% 599/947 [06:33<03:42,  1.57it/s]\u001b[A\n",
            "Iteration:  63% 600/947 [06:34<03:42,  1.56it/s]\u001b[A\n",
            "Iteration:  63% 601/947 [06:35<03:41,  1.56it/s]\u001b[A\n",
            "Iteration:  64% 602/947 [06:35<03:40,  1.57it/s]\u001b[A\n",
            "Iteration:  64% 603/947 [06:36<03:40,  1.56it/s]\u001b[A\n",
            "Iteration:  64% 604/947 [06:36<03:39,  1.57it/s]\u001b[A\n",
            "Iteration:  64% 605/947 [06:37<03:39,  1.56it/s]\u001b[A\n",
            "Iteration:  64% 606/947 [06:38<03:37,  1.57it/s]\u001b[A\n",
            "Iteration:  64% 607/947 [06:38<03:37,  1.56it/s]\u001b[A\n",
            "Iteration:  64% 608/947 [06:39<03:36,  1.57it/s]\u001b[A\n",
            "Iteration:  64% 609/947 [06:40<03:35,  1.56it/s]\u001b[A\n",
            "Iteration:  64% 610/947 [06:40<03:34,  1.57it/s]\u001b[A\n",
            "Iteration:  65% 611/947 [06:41<03:34,  1.57it/s]\u001b[A\n",
            "Iteration:  65% 612/947 [06:42<03:33,  1.57it/s]\u001b[A\n",
            "Iteration:  65% 613/947 [06:42<03:33,  1.57it/s]\u001b[A\n",
            "Iteration:  65% 614/947 [06:43<03:32,  1.57it/s]\u001b[A\n",
            "Iteration:  65% 615/947 [06:44<03:32,  1.57it/s]\u001b[A\n",
            "Iteration:  65% 616/947 [06:44<03:31,  1.57it/s]\u001b[A\n",
            "Iteration:  65% 617/947 [06:45<03:30,  1.57it/s]\u001b[A\n",
            "Iteration:  65% 618/947 [06:45<03:29,  1.57it/s]\u001b[A\n",
            "Iteration:  65% 619/947 [06:46<03:29,  1.57it/s]\u001b[A\n",
            "Iteration:  65% 620/947 [06:47<03:29,  1.56it/s]\u001b[A\n",
            "Iteration:  66% 621/947 [06:47<03:28,  1.57it/s]\u001b[A\n",
            "Iteration:  66% 622/947 [06:48<03:27,  1.57it/s]\u001b[A\n",
            "Iteration:  66% 623/947 [06:49<03:26,  1.57it/s]\u001b[A\n",
            "Iteration:  66% 624/947 [06:49<03:26,  1.57it/s]\u001b[A\n",
            "Iteration:  66% 625/947 [06:50<03:26,  1.56it/s]\u001b[A\n",
            "Iteration:  66% 626/947 [06:51<03:25,  1.56it/s]\u001b[A\n",
            "Iteration:  66% 627/947 [06:51<03:25,  1.56it/s]\u001b[A\n",
            "Iteration:  66% 628/947 [06:52<03:23,  1.56it/s]\u001b[A\n",
            "Iteration:  66% 629/947 [06:52<03:23,  1.56it/s]\u001b[A\n",
            "Iteration:  67% 630/947 [06:53<03:22,  1.57it/s]\u001b[A\n",
            "Iteration:  67% 631/947 [06:54<03:22,  1.56it/s]\u001b[A\n",
            "Iteration:  67% 632/947 [06:54<03:21,  1.57it/s]\u001b[A\n",
            "Iteration:  67% 633/947 [06:55<03:20,  1.56it/s]\u001b[A\n",
            "Iteration:  67% 634/947 [06:56<03:19,  1.57it/s]\u001b[A\n",
            "Iteration:  67% 635/947 [06:56<03:19,  1.57it/s]\u001b[A\n",
            "Iteration:  67% 636/947 [06:57<03:18,  1.56it/s]\u001b[A\n",
            "Iteration:  67% 637/947 [06:58<03:18,  1.56it/s]\u001b[A\n",
            "Iteration:  67% 638/947 [06:58<03:17,  1.56it/s]\u001b[A\n",
            "Iteration:  67% 639/947 [06:59<03:17,  1.56it/s]\u001b[A\n",
            "Iteration:  68% 640/947 [06:59<03:16,  1.56it/s]\u001b[A\n",
            "Iteration:  68% 641/947 [07:00<03:15,  1.56it/s]\u001b[A\n",
            "Iteration:  68% 642/947 [07:01<03:15,  1.56it/s]\u001b[A\n",
            "Iteration:  68% 643/947 [07:01<03:14,  1.56it/s]\u001b[A\n",
            "Iteration:  68% 644/947 [07:02<03:14,  1.56it/s]\u001b[A\n",
            "Iteration:  68% 645/947 [07:03<03:13,  1.56it/s]\u001b[A\n",
            "Iteration:  68% 646/947 [07:03<03:12,  1.56it/s]\u001b[A\n",
            "Iteration:  68% 647/947 [07:04<03:12,  1.56it/s]\u001b[A\n",
            "Iteration:  68% 648/947 [07:05<03:11,  1.56it/s]\u001b[A\n",
            "Iteration:  69% 649/947 [07:05<03:10,  1.56it/s]\u001b[A\n",
            "Iteration:  69% 650/947 [07:06<03:10,  1.56it/s]\u001b[A\n",
            "Iteration:  69% 651/947 [07:07<03:10,  1.56it/s]\u001b[A\n",
            "Iteration:  69% 652/947 [07:07<03:09,  1.56it/s]\u001b[A\n",
            "Iteration:  69% 653/947 [07:08<03:08,  1.56it/s]\u001b[A\n",
            "Iteration:  69% 654/947 [07:08<03:08,  1.56it/s]\u001b[A\n",
            "Iteration:  69% 655/947 [07:09<03:07,  1.55it/s]\u001b[A\n",
            "Iteration:  69% 656/947 [07:10<03:07,  1.55it/s]\u001b[A\n",
            "Iteration:  69% 657/947 [07:10<03:06,  1.56it/s]\u001b[A\n",
            "Iteration:  69% 658/947 [07:11<03:05,  1.56it/s]\u001b[A\n",
            "Iteration:  70% 659/947 [07:12<03:05,  1.55it/s]\u001b[A\n",
            "Iteration:  70% 660/947 [07:12<03:04,  1.56it/s]\u001b[A\n",
            "Iteration:  70% 661/947 [07:13<03:03,  1.56it/s]\u001b[A\n",
            "Iteration:  70% 662/947 [07:14<03:02,  1.56it/s]\u001b[A\n",
            "Iteration:  70% 663/947 [07:14<03:02,  1.56it/s]\u001b[A\n",
            "Iteration:  70% 664/947 [07:15<03:01,  1.56it/s]\u001b[A\n",
            "Iteration:  70% 665/947 [07:16<03:00,  1.56it/s]\u001b[A\n",
            "Iteration:  70% 666/947 [07:16<03:00,  1.56it/s]\u001b[A\n",
            "Iteration:  70% 667/947 [07:17<02:59,  1.56it/s]\u001b[A\n",
            "Iteration:  71% 668/947 [07:17<02:59,  1.55it/s]\u001b[A\n",
            "Iteration:  71% 669/947 [07:18<02:58,  1.55it/s]\u001b[A\n",
            "Iteration:  71% 670/947 [07:19<02:58,  1.55it/s]\u001b[A\n",
            "Iteration:  71% 671/947 [07:19<02:57,  1.56it/s]\u001b[A\n",
            "Iteration:  71% 672/947 [07:20<02:57,  1.55it/s]\u001b[A\n",
            "Iteration:  71% 673/947 [07:21<02:56,  1.55it/s]\u001b[A\n",
            "Iteration:  71% 674/947 [07:21<02:55,  1.55it/s]\u001b[A\n",
            "Iteration:  71% 675/947 [07:22<02:55,  1.55it/s]\u001b[A\n",
            "Iteration:  71% 676/947 [07:23<02:54,  1.55it/s]\u001b[A\n",
            "Iteration:  71% 677/947 [07:23<02:53,  1.55it/s]\u001b[A\n",
            "Iteration:  72% 678/947 [07:24<02:53,  1.55it/s]\u001b[A\n",
            "Iteration:  72% 679/947 [07:25<02:52,  1.55it/s]\u001b[A\n",
            "Iteration:  72% 680/947 [07:25<02:52,  1.55it/s]\u001b[A\n",
            "Iteration:  72% 681/947 [07:26<02:51,  1.55it/s]\u001b[A\n",
            "Iteration:  72% 682/947 [07:26<02:51,  1.55it/s]\u001b[A\n",
            "Iteration:  72% 683/947 [07:27<02:50,  1.55it/s]\u001b[A\n",
            "Iteration:  72% 684/947 [07:28<02:49,  1.55it/s]\u001b[A\n",
            "Iteration:  72% 685/947 [07:28<02:48,  1.55it/s]\u001b[A\n",
            "Iteration:  72% 686/947 [07:29<02:48,  1.55it/s]\u001b[A\n",
            "Iteration:  73% 687/947 [07:30<02:47,  1.55it/s]\u001b[A\n",
            "Iteration:  73% 688/947 [07:30<02:46,  1.56it/s]\u001b[A\n",
            "Iteration:  73% 689/947 [07:31<02:45,  1.55it/s]\u001b[A\n",
            "Iteration:  73% 690/947 [07:32<02:45,  1.56it/s]\u001b[A\n",
            "Iteration:  73% 691/947 [07:32<02:44,  1.55it/s]\u001b[A\n",
            "Iteration:  73% 692/947 [07:33<02:43,  1.56it/s]\u001b[A\n",
            "Iteration:  73% 693/947 [07:34<02:43,  1.56it/s]\u001b[A\n",
            "Iteration:  73% 694/947 [07:34<02:42,  1.56it/s]\u001b[A\n",
            "Iteration:  73% 695/947 [07:35<02:42,  1.55it/s]\u001b[A\n",
            "Iteration:  73% 696/947 [07:35<02:41,  1.56it/s]\u001b[A\n",
            "Iteration:  74% 697/947 [07:36<02:40,  1.56it/s]\u001b[A\n",
            "Iteration:  74% 698/947 [07:37<02:40,  1.56it/s]\u001b[A\n",
            "Iteration:  74% 699/947 [07:37<02:39,  1.56it/s]\u001b[A\n",
            "Iteration:  74% 700/947 [07:38<02:38,  1.56it/s]\u001b[A\n",
            "Iteration:  74% 701/947 [07:39<02:38,  1.55it/s]\u001b[A\n",
            "Iteration:  74% 702/947 [07:39<02:38,  1.54it/s]\u001b[A\n",
            "Iteration:  74% 703/947 [07:40<02:36,  1.56it/s]\u001b[A\n",
            "Iteration:  74% 704/947 [07:41<02:35,  1.56it/s]\u001b[A\n",
            "Iteration:  74% 705/947 [07:41<02:35,  1.56it/s]\u001b[A\n",
            "Iteration:  75% 706/947 [07:42<02:34,  1.56it/s]\u001b[A\n",
            "Iteration:  75% 707/947 [07:43<02:34,  1.56it/s]\u001b[A\n",
            "Iteration:  75% 708/947 [07:43<02:33,  1.56it/s]\u001b[A\n",
            "Iteration:  75% 709/947 [07:44<02:32,  1.56it/s]\u001b[A\n",
            "Iteration:  75% 710/947 [07:44<02:32,  1.56it/s]\u001b[A\n",
            "Iteration:  75% 711/947 [07:45<02:31,  1.56it/s]\u001b[A\n",
            "Iteration:  75% 712/947 [07:46<02:30,  1.56it/s]\u001b[A\n",
            "Iteration:  75% 713/947 [07:46<02:29,  1.56it/s]\u001b[A\n",
            "Iteration:  75% 714/947 [07:47<02:29,  1.56it/s]\u001b[A\n",
            "Iteration:  76% 715/947 [07:48<02:28,  1.56it/s]\u001b[A\n",
            "Iteration:  76% 716/947 [07:48<02:28,  1.56it/s]\u001b[A\n",
            "Iteration:  76% 717/947 [07:49<02:27,  1.56it/s]\u001b[A\n",
            "Iteration:  76% 718/947 [07:50<02:26,  1.56it/s]\u001b[A\n",
            "Iteration:  76% 719/947 [07:50<02:25,  1.56it/s]\u001b[A\n",
            "Iteration:  76% 720/947 [07:51<02:25,  1.56it/s]\u001b[A\n",
            "Iteration:  76% 721/947 [07:52<02:24,  1.56it/s]\u001b[A\n",
            "Iteration:  76% 722/947 [07:52<02:24,  1.56it/s]\u001b[A\n",
            "Iteration:  76% 723/947 [07:53<02:23,  1.56it/s]\u001b[A\n",
            "Iteration:  76% 724/947 [07:53<02:23,  1.56it/s]\u001b[A\n",
            "Iteration:  77% 725/947 [07:54<02:22,  1.56it/s]\u001b[A\n",
            "Iteration:  77% 726/947 [07:55<02:21,  1.56it/s]\u001b[A\n",
            "Iteration:  77% 727/947 [07:55<02:21,  1.56it/s]\u001b[A\n",
            "Iteration:  77% 728/947 [07:56<02:20,  1.56it/s]\u001b[A\n",
            "Iteration:  77% 729/947 [07:57<02:19,  1.56it/s]\u001b[A\n",
            "Iteration:  77% 730/947 [07:57<02:18,  1.56it/s]\u001b[A\n",
            "Iteration:  77% 731/947 [07:58<02:18,  1.56it/s]\u001b[A\n",
            "Iteration:  77% 732/947 [07:59<02:17,  1.56it/s]\u001b[A\n",
            "Iteration:  77% 733/947 [07:59<02:17,  1.56it/s]\u001b[A\n",
            "Iteration:  78% 734/947 [08:00<02:16,  1.56it/s]\u001b[A\n",
            "Iteration:  78% 735/947 [08:00<02:15,  1.56it/s]\u001b[A\n",
            "Iteration:  78% 736/947 [08:01<02:15,  1.56it/s]\u001b[A\n",
            "Iteration:  78% 737/947 [08:02<02:14,  1.56it/s]\u001b[A\n",
            "Iteration:  78% 738/947 [08:02<02:13,  1.56it/s]\u001b[A\n",
            "Iteration:  78% 739/947 [08:03<02:12,  1.57it/s]\u001b[A\n",
            "Iteration:  78% 740/947 [08:04<02:12,  1.56it/s]\u001b[A\n",
            "Iteration:  78% 741/947 [08:04<02:11,  1.57it/s]\u001b[A\n",
            "Iteration:  78% 742/947 [08:05<02:11,  1.56it/s]\u001b[A\n",
            "Iteration:  78% 743/947 [08:06<02:10,  1.56it/s]\u001b[A\n",
            "Iteration:  79% 744/947 [08:06<02:09,  1.56it/s]\u001b[A\n",
            "Iteration:  79% 745/947 [08:07<02:09,  1.56it/s]\u001b[A\n",
            "Iteration:  79% 746/947 [08:08<02:08,  1.56it/s]\u001b[A\n",
            "Iteration:  79% 747/947 [08:08<02:08,  1.56it/s]\u001b[A\n",
            "Iteration:  79% 748/947 [08:09<02:07,  1.56it/s]\u001b[A\n",
            "Iteration:  79% 749/947 [08:09<02:06,  1.56it/s]\u001b[A\n",
            "Iteration:  79% 750/947 [08:10<02:06,  1.56it/s]\u001b[A\n",
            "Iteration:  79% 751/947 [08:11<02:05,  1.56it/s]\u001b[A\n",
            "Iteration:  79% 752/947 [08:11<02:04,  1.56it/s]\u001b[A\n",
            "Iteration:  80% 753/947 [08:12<02:04,  1.56it/s]\u001b[A\n",
            "Iteration:  80% 754/947 [08:13<02:03,  1.56it/s]\u001b[A\n",
            "Iteration:  80% 755/947 [08:13<02:02,  1.56it/s]\u001b[A\n",
            "Iteration:  80% 756/947 [08:14<02:02,  1.56it/s]\u001b[A\n",
            "Iteration:  80% 757/947 [08:15<02:01,  1.56it/s]\u001b[A\n",
            "Iteration:  80% 758/947 [08:15<02:01,  1.56it/s]\u001b[A\n",
            "Iteration:  80% 759/947 [08:16<02:00,  1.56it/s]\u001b[A\n",
            "Iteration:  80% 760/947 [08:17<01:59,  1.56it/s]\u001b[A\n",
            "Iteration:  80% 761/947 [08:17<01:59,  1.56it/s]\u001b[A\n",
            "Iteration:  80% 762/947 [08:18<01:58,  1.56it/s]\u001b[A\n",
            "Iteration:  81% 763/947 [08:18<01:57,  1.56it/s]\u001b[A\n",
            "Iteration:  81% 764/947 [08:19<01:57,  1.56it/s]\u001b[A\n",
            "Iteration:  81% 765/947 [08:20<01:56,  1.56it/s]\u001b[A\n",
            "Iteration:  81% 766/947 [08:20<01:55,  1.56it/s]\u001b[A\n",
            "Iteration:  81% 767/947 [08:21<01:55,  1.56it/s]\u001b[A\n",
            "Iteration:  81% 768/947 [08:22<01:54,  1.56it/s]\u001b[A\n",
            "Iteration:  81% 769/947 [08:22<01:54,  1.56it/s]\u001b[A\n",
            "Iteration:  81% 770/947 [08:23<01:53,  1.55it/s]\u001b[A\n",
            "Iteration:  81% 771/947 [08:24<01:52,  1.56it/s]\u001b[A\n",
            "Iteration:  82% 772/947 [08:24<01:52,  1.56it/s]\u001b[A\n",
            "Iteration:  82% 773/947 [08:25<01:51,  1.56it/s]\u001b[A\n",
            "Iteration:  82% 774/947 [08:25<01:50,  1.56it/s]\u001b[A\n",
            "Iteration:  82% 775/947 [08:26<01:50,  1.56it/s]\u001b[A\n",
            "Iteration:  82% 776/947 [08:27<01:49,  1.56it/s]\u001b[A\n",
            "Iteration:  82% 777/947 [08:27<01:48,  1.56it/s]\u001b[A\n",
            "Iteration:  82% 778/947 [08:28<01:48,  1.56it/s]\u001b[A\n",
            "Iteration:  82% 779/947 [08:29<01:47,  1.56it/s]\u001b[A\n",
            "Iteration:  82% 780/947 [08:29<01:47,  1.56it/s]\u001b[A\n",
            "Iteration:  82% 781/947 [08:30<01:46,  1.56it/s]\u001b[A\n",
            "Iteration:  83% 782/947 [08:31<01:45,  1.56it/s]\u001b[A\n",
            "Iteration:  83% 783/947 [08:31<01:45,  1.56it/s]\u001b[A\n",
            "Iteration:  83% 784/947 [08:32<01:44,  1.56it/s]\u001b[A\n",
            "Iteration:  83% 785/947 [08:33<01:43,  1.56it/s]\u001b[A\n",
            "Iteration:  83% 786/947 [08:33<01:43,  1.56it/s]\u001b[A\n",
            "Iteration:  83% 787/947 [08:34<01:42,  1.56it/s]\u001b[A\n",
            "Iteration:  83% 788/947 [08:34<01:42,  1.56it/s]\u001b[A\n",
            "Iteration:  83% 789/947 [08:35<01:41,  1.56it/s]\u001b[A\n",
            "Iteration:  83% 790/947 [08:36<01:40,  1.56it/s]\u001b[A\n",
            "Iteration:  84% 791/947 [08:36<01:40,  1.56it/s]\u001b[A\n",
            "Iteration:  84% 792/947 [08:37<01:39,  1.56it/s]\u001b[A\n",
            "Iteration:  84% 793/947 [08:38<01:38,  1.56it/s]\u001b[A\n",
            "Iteration:  84% 794/947 [08:38<01:37,  1.56it/s]\u001b[A\n",
            "Iteration:  84% 795/947 [08:39<01:37,  1.56it/s]\u001b[A\n",
            "Iteration:  84% 796/947 [08:40<01:36,  1.56it/s]\u001b[A\n",
            "Iteration:  84% 797/947 [08:40<01:36,  1.56it/s]\u001b[A\n",
            "Iteration:  84% 798/947 [08:41<01:35,  1.56it/s]\u001b[A\n",
            "Iteration:  84% 799/947 [08:42<01:34,  1.56it/s]\u001b[A\n",
            "Iteration:  84% 800/947 [08:42<01:34,  1.56it/s]\u001b[A\n",
            "Iteration:  85% 801/947 [08:43<01:33,  1.56it/s]\u001b[A\n",
            "Iteration:  85% 802/947 [08:43<01:32,  1.56it/s]\u001b[A\n",
            "Iteration:  85% 803/947 [08:44<01:32,  1.56it/s]\u001b[A\n",
            "Iteration:  85% 804/947 [08:45<01:31,  1.56it/s]\u001b[A\n",
            "Iteration:  85% 805/947 [08:45<01:31,  1.56it/s]\u001b[A\n",
            "Iteration:  85% 806/947 [08:46<01:30,  1.56it/s]\u001b[A\n",
            "Iteration:  85% 807/947 [08:47<01:29,  1.56it/s]\u001b[A\n",
            "Iteration:  85% 808/947 [08:47<01:29,  1.56it/s]\u001b[A\n",
            "Iteration:  85% 809/947 [08:48<01:28,  1.56it/s]\u001b[A\n",
            "Iteration:  86% 810/947 [08:49<01:27,  1.56it/s]\u001b[A\n",
            "Iteration:  86% 811/947 [08:49<01:27,  1.56it/s]\u001b[A\n",
            "Iteration:  86% 812/947 [08:50<01:26,  1.56it/s]\u001b[A\n",
            "Iteration:  86% 813/947 [08:50<01:26,  1.56it/s]\u001b[A\n",
            "Iteration:  86% 814/947 [08:51<01:25,  1.56it/s]\u001b[A\n",
            "Iteration:  86% 815/947 [08:52<01:24,  1.56it/s]\u001b[A\n",
            "Iteration:  86% 816/947 [08:52<01:23,  1.56it/s]\u001b[A\n",
            "Iteration:  86% 817/947 [08:53<01:23,  1.56it/s]\u001b[A\n",
            "Iteration:  86% 818/947 [08:54<01:22,  1.56it/s]\u001b[A\n",
            "Iteration:  86% 819/947 [08:54<01:22,  1.55it/s]\u001b[A\n",
            "Iteration:  87% 820/947 [08:55<01:21,  1.56it/s]\u001b[A\n",
            "Iteration:  87% 821/947 [08:56<01:20,  1.56it/s]\u001b[A\n",
            "Iteration:  87% 822/947 [08:56<01:20,  1.56it/s]\u001b[A\n",
            "Iteration:  87% 823/947 [08:57<01:19,  1.56it/s]\u001b[A\n",
            "Iteration:  87% 824/947 [08:58<01:18,  1.56it/s]\u001b[A\n",
            "Iteration:  87% 825/947 [08:58<01:18,  1.56it/s]\u001b[A\n",
            "Iteration:  87% 826/947 [08:59<01:17,  1.56it/s]\u001b[A\n",
            "Iteration:  87% 827/947 [08:59<01:16,  1.56it/s]\u001b[A\n",
            "Iteration:  87% 828/947 [09:00<01:16,  1.56it/s]\u001b[A\n",
            "Iteration:  88% 829/947 [09:01<01:15,  1.56it/s]\u001b[A\n",
            "Iteration:  88% 830/947 [09:01<01:15,  1.56it/s]\u001b[A\n",
            "Iteration:  88% 831/947 [09:02<01:14,  1.56it/s]\u001b[A\n",
            "Iteration:  88% 832/947 [09:03<01:13,  1.56it/s]\u001b[A\n",
            "Iteration:  88% 833/947 [09:03<01:13,  1.56it/s]\u001b[A\n",
            "Iteration:  88% 834/947 [09:04<01:12,  1.56it/s]\u001b[A\n",
            "Iteration:  88% 835/947 [09:05<01:11,  1.56it/s]\u001b[A\n",
            "Iteration:  88% 836/947 [09:05<01:11,  1.56it/s]\u001b[A\n",
            "Iteration:  88% 837/947 [09:06<01:10,  1.56it/s]\u001b[A\n",
            "Iteration:  88% 838/947 [09:07<01:09,  1.56it/s]\u001b[A\n",
            "Iteration:  89% 839/947 [09:07<01:09,  1.56it/s]\u001b[A\n",
            "Iteration:  89% 840/947 [09:08<01:08,  1.55it/s]\u001b[A\n",
            "Iteration:  89% 841/947 [09:08<01:08,  1.56it/s]\u001b[A\n",
            "Iteration:  89% 842/947 [09:09<01:07,  1.56it/s]\u001b[A\n",
            "Iteration:  89% 843/947 [09:10<01:06,  1.56it/s]\u001b[A\n",
            "Iteration:  89% 844/947 [09:10<01:06,  1.56it/s]\u001b[A\n",
            "Iteration:  89% 845/947 [09:11<01:05,  1.56it/s]\u001b[A\n",
            "Iteration:  89% 846/947 [09:12<01:04,  1.56it/s]\u001b[A\n",
            "Iteration:  89% 847/947 [09:12<01:04,  1.56it/s]\u001b[A\n",
            "Iteration:  90% 848/947 [09:13<01:03,  1.56it/s]\u001b[A\n",
            "Iteration:  90% 849/947 [09:14<01:02,  1.56it/s]\u001b[A\n",
            "Iteration:  90% 850/947 [09:14<01:02,  1.56it/s]\u001b[A\n",
            "Iteration:  90% 851/947 [09:15<01:01,  1.56it/s]\u001b[A\n",
            "Iteration:  90% 852/947 [09:16<01:00,  1.56it/s]\u001b[A\n",
            "Iteration:  90% 853/947 [09:16<01:00,  1.56it/s]\u001b[A\n",
            "Iteration:  90% 854/947 [09:17<00:59,  1.56it/s]\u001b[A\n",
            "Iteration:  90% 855/947 [09:17<00:59,  1.56it/s]\u001b[A\n",
            "Iteration:  90% 856/947 [09:18<00:58,  1.56it/s]\u001b[A\n",
            "Iteration:  90% 857/947 [09:19<00:57,  1.56it/s]\u001b[A\n",
            "Iteration:  91% 858/947 [09:19<00:57,  1.56it/s]\u001b[A\n",
            "Iteration:  91% 859/947 [09:20<00:56,  1.56it/s]\u001b[A\n",
            "Iteration:  91% 860/947 [09:21<00:55,  1.55it/s]\u001b[A\n",
            "Iteration:  91% 861/947 [09:21<00:55,  1.56it/s]\u001b[A\n",
            "Iteration:  91% 862/947 [09:22<00:54,  1.56it/s]\u001b[A\n",
            "Iteration:  91% 863/947 [09:23<00:53,  1.56it/s]\u001b[A\n",
            "Iteration:  91% 864/947 [09:23<00:53,  1.56it/s]\u001b[A\n",
            "Iteration:  91% 865/947 [09:24<00:52,  1.56it/s]\u001b[A\n",
            "Iteration:  91% 866/947 [09:24<00:52,  1.56it/s]\u001b[A\n",
            "Iteration:  92% 867/947 [09:25<00:51,  1.55it/s]\u001b[A\n",
            "Iteration:  92% 868/947 [09:26<00:50,  1.56it/s]\u001b[A\n",
            "Iteration:  92% 869/947 [09:26<00:50,  1.56it/s]\u001b[A\n",
            "Iteration:  92% 870/947 [09:27<00:49,  1.56it/s]\u001b[A\n",
            "Iteration:  92% 871/947 [09:28<00:48,  1.56it/s]\u001b[A\n",
            "Iteration:  92% 872/947 [09:28<00:48,  1.56it/s]\u001b[A\n",
            "Iteration:  92% 873/947 [09:29<00:47,  1.56it/s]\u001b[A\n",
            "Iteration:  92% 874/947 [09:30<00:46,  1.56it/s]\u001b[A\n",
            "Iteration:  92% 875/947 [09:30<00:46,  1.56it/s]\u001b[A\n",
            "Iteration:  93% 876/947 [09:31<00:45,  1.56it/s]\u001b[A\n",
            "Iteration:  93% 877/947 [09:32<00:44,  1.56it/s]\u001b[A\n",
            "Iteration:  93% 878/947 [09:32<00:44,  1.56it/s]\u001b[A\n",
            "Iteration:  93% 879/947 [09:33<00:43,  1.56it/s]\u001b[A\n",
            "Iteration:  93% 880/947 [09:33<00:43,  1.56it/s]\u001b[A\n",
            "Iteration:  93% 881/947 [09:34<00:42,  1.56it/s]\u001b[A\n",
            "Iteration:  93% 882/947 [09:35<00:41,  1.56it/s]\u001b[A\n",
            "Iteration:  93% 883/947 [09:35<00:41,  1.56it/s]\u001b[A\n",
            "Iteration:  93% 884/947 [09:36<00:40,  1.56it/s]\u001b[A\n",
            "Iteration:  93% 885/947 [09:37<00:39,  1.56it/s]\u001b[A\n",
            "Iteration:  94% 886/947 [09:37<00:39,  1.56it/s]\u001b[A\n",
            "Iteration:  94% 887/947 [09:38<00:38,  1.55it/s]\u001b[A\n",
            "Iteration:  94% 888/947 [09:39<00:37,  1.56it/s]\u001b[A\n",
            "Iteration:  94% 889/947 [09:39<00:37,  1.56it/s]\u001b[A\n",
            "Iteration:  94% 890/947 [09:40<00:36,  1.55it/s]\u001b[A\n",
            "Iteration:  94% 891/947 [09:41<00:36,  1.55it/s]\u001b[A\n",
            "Iteration:  94% 892/947 [09:41<00:35,  1.56it/s]\u001b[A\n",
            "Iteration:  94% 893/947 [09:42<00:34,  1.56it/s]\u001b[A\n",
            "Iteration:  94% 894/947 [09:42<00:34,  1.56it/s]\u001b[A\n",
            "Iteration:  95% 895/947 [09:43<00:33,  1.56it/s]\u001b[A\n",
            "Iteration:  95% 896/947 [09:44<00:32,  1.56it/s]\u001b[A\n",
            "Iteration:  95% 897/947 [09:44<00:32,  1.56it/s]\u001b[A\n",
            "Iteration:  95% 898/947 [09:45<00:31,  1.56it/s]\u001b[A\n",
            "Iteration:  95% 899/947 [09:46<00:30,  1.56it/s]\u001b[A\n",
            "Iteration:  95% 900/947 [09:46<00:30,  1.56it/s]\u001b[A\n",
            "Iteration:  95% 901/947 [09:47<00:29,  1.55it/s]\u001b[A\n",
            "Iteration:  95% 902/947 [09:48<00:28,  1.56it/s]\u001b[A\n",
            "Iteration:  95% 903/947 [09:48<00:28,  1.56it/s]\u001b[A\n",
            "Iteration:  95% 904/947 [09:49<00:27,  1.56it/s]\u001b[A\n",
            "Iteration:  96% 905/947 [09:50<00:26,  1.56it/s]\u001b[A\n",
            "Iteration:  96% 906/947 [09:50<00:26,  1.56it/s]\u001b[A\n",
            "Iteration:  96% 907/947 [09:51<00:25,  1.56it/s]\u001b[A\n",
            "Iteration:  96% 908/947 [09:51<00:24,  1.56it/s]\u001b[A\n",
            "Iteration:  96% 909/947 [09:52<00:24,  1.56it/s]\u001b[A\n",
            "Iteration:  96% 910/947 [09:53<00:23,  1.56it/s]\u001b[A\n",
            "Iteration:  96% 911/947 [09:53<00:23,  1.56it/s]\u001b[A\n",
            "Iteration:  96% 912/947 [09:54<00:22,  1.56it/s]\u001b[A\n",
            "Iteration:  96% 913/947 [09:55<00:21,  1.56it/s]\u001b[A\n",
            "Iteration:  97% 914/947 [09:55<00:21,  1.56it/s]\u001b[A\n",
            "Iteration:  97% 915/947 [09:56<00:20,  1.56it/s]\u001b[A\n",
            "Iteration:  97% 916/947 [09:57<00:19,  1.56it/s]\u001b[A\n",
            "Iteration:  97% 917/947 [09:57<00:19,  1.56it/s]\u001b[A\n",
            "Iteration:  97% 918/947 [09:58<00:18,  1.56it/s]\u001b[A\n",
            "Iteration:  97% 919/947 [09:59<00:17,  1.56it/s]\u001b[A\n",
            "Iteration:  97% 920/947 [09:59<00:17,  1.56it/s]\u001b[A\n",
            "Iteration:  97% 921/947 [10:00<00:16,  1.56it/s]\u001b[A\n",
            "Iteration:  97% 922/947 [10:00<00:16,  1.56it/s]\u001b[A\n",
            "Iteration:  97% 923/947 [10:01<00:15,  1.56it/s]\u001b[A\n",
            "Iteration:  98% 924/947 [10:02<00:14,  1.56it/s]\u001b[A\n",
            "Iteration:  98% 925/947 [10:02<00:14,  1.56it/s]\u001b[A\n",
            "Iteration:  98% 926/947 [10:03<00:13,  1.56it/s]\u001b[A\n",
            "Iteration:  98% 927/947 [10:04<00:12,  1.56it/s]\u001b[A\n",
            "Iteration:  98% 928/947 [10:04<00:12,  1.56it/s]\u001b[A\n",
            "Iteration:  98% 929/947 [10:05<00:11,  1.56it/s]\u001b[A\n",
            "Iteration:  98% 930/947 [10:06<00:10,  1.56it/s]\u001b[A\n",
            "Iteration:  98% 931/947 [10:06<00:10,  1.56it/s]\u001b[A\n",
            "Iteration:  98% 932/947 [10:07<00:09,  1.56it/s]\u001b[A\n",
            "Iteration:  99% 933/947 [10:07<00:08,  1.56it/s]\u001b[A\n",
            "Iteration:  99% 934/947 [10:08<00:08,  1.56it/s]\u001b[A\n",
            "Iteration:  99% 935/947 [10:09<00:07,  1.56it/s]\u001b[A\n",
            "Iteration:  99% 936/947 [10:09<00:07,  1.56it/s]\u001b[A\n",
            "Iteration:  99% 937/947 [10:10<00:06,  1.56it/s]\u001b[A\n",
            "Iteration:  99% 938/947 [10:11<00:05,  1.56it/s]\u001b[A\n",
            "Iteration:  99% 939/947 [10:11<00:05,  1.56it/s]\u001b[A\n",
            "Iteration:  99% 940/947 [10:12<00:04,  1.56it/s]\u001b[A\n",
            "Iteration:  99% 941/947 [10:13<00:03,  1.56it/s]\u001b[A\n",
            "Iteration:  99% 942/947 [10:13<00:03,  1.56it/s]\u001b[A\n",
            "Iteration: 100% 943/947 [10:14<00:02,  1.56it/s]\u001b[A\n",
            "Iteration: 100% 944/947 [10:15<00:01,  1.56it/s]\u001b[A\n",
            "Iteration: 100% 945/947 [10:15<00:01,  1.56it/s]\u001b[A\n",
            "Iteration: 100% 946/947 [10:16<00:00,  1.56it/s]\u001b[A\n",
            "Iteration: 100% 947/947 [10:16<00:00,  1.53it/s]\n",
            "Epoch:  20% 1/5 [10:16<41:07, 616.95s/it]\n",
            "Iteration:   0% 0/947 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/947 [00:00<10:06,  1.56it/s]\u001b[A\n",
            "Iteration:   0% 2/947 [00:01<10:05,  1.56it/s]\u001b[A\n",
            "Iteration:   0% 3/947 [00:01<10:06,  1.56it/s]\u001b[A\n",
            "Iteration:   0% 4/947 [00:02<10:05,  1.56it/s]\u001b[A\n",
            "Iteration:   1% 5/947 [00:03<10:04,  1.56it/s]\u001b[A\n",
            "Iteration:   1% 6/947 [00:03<10:02,  1.56it/s]\u001b[A\n",
            "Iteration:   1% 7/947 [00:04<10:02,  1.56it/s]\u001b[A\n",
            "Iteration:   1% 8/947 [00:05<10:01,  1.56it/s]\u001b[A\n",
            "Iteration:   1% 9/947 [00:05<10:00,  1.56it/s]\u001b[A\n",
            "Iteration:   1% 10/947 [00:06<10:00,  1.56it/s]\u001b[A\n",
            "Iteration:   1% 11/947 [00:07<09:58,  1.56it/s]\u001b[A\n",
            "Iteration:   1% 12/947 [00:07<09:58,  1.56it/s]\u001b[A\n",
            "Iteration:   1% 13/947 [00:08<09:58,  1.56it/s]\u001b[A\n",
            "Iteration:   1% 14/947 [00:08<09:58,  1.56it/s]\u001b[A\n",
            "Iteration:   2% 15/947 [00:09<09:57,  1.56it/s]\u001b[A\n",
            "Iteration:   2% 16/947 [00:10<09:57,  1.56it/s]\u001b[A\n",
            "Iteration:   2% 17/947 [00:10<09:56,  1.56it/s]\u001b[A\n",
            "Iteration:   2% 18/947 [00:11<09:55,  1.56it/s]\u001b[A\n",
            "Iteration:   2% 19/947 [00:12<09:55,  1.56it/s]\u001b[A\n",
            "Iteration:   2% 20/947 [00:12<09:54,  1.56it/s]\u001b[A\n",
            "Iteration:   2% 21/947 [00:13<09:53,  1.56it/s]\u001b[A\n",
            "Iteration:   2% 22/947 [00:14<09:52,  1.56it/s]\u001b[A\n",
            "Iteration:   2% 23/947 [00:14<09:52,  1.56it/s]\u001b[A\n",
            "Iteration:   3% 24/947 [00:15<09:51,  1.56it/s]\u001b[A\n",
            "Iteration:   3% 25/947 [00:16<09:51,  1.56it/s]\u001b[A\n",
            "Iteration:   3% 26/947 [00:16<09:51,  1.56it/s]\u001b[A\n",
            "Iteration:   3% 27/947 [00:17<09:49,  1.56it/s]\u001b[A\n",
            "Iteration:   3% 28/947 [00:17<09:48,  1.56it/s]\u001b[A\n",
            "Iteration:   3% 29/947 [00:18<09:47,  1.56it/s]\u001b[A\n",
            "Iteration:   3% 30/947 [00:19<09:47,  1.56it/s]\u001b[A\n",
            "Iteration:   3% 31/947 [00:19<09:47,  1.56it/s]\u001b[A\n",
            "Iteration:   3% 32/947 [00:20<09:45,  1.56it/s]\u001b[A\n",
            "Iteration:   3% 33/947 [00:21<09:47,  1.56it/s]\u001b[A\n",
            "Iteration:   4% 34/947 [00:21<09:45,  1.56it/s]\u001b[A\n",
            "Iteration:   4% 35/947 [00:22<09:45,  1.56it/s]\u001b[A\n",
            "Iteration:   4% 36/947 [00:23<09:44,  1.56it/s]\u001b[A\n",
            "Iteration:   4% 37/947 [00:23<09:43,  1.56it/s]\u001b[A\n",
            "Iteration:   4% 38/947 [00:24<09:42,  1.56it/s]\u001b[A\n",
            "Iteration:   4% 39/947 [00:25<09:41,  1.56it/s]\u001b[A\n",
            "Iteration:   4% 40/947 [00:25<09:41,  1.56it/s]\u001b[A\n",
            "Iteration:   4% 41/947 [00:26<09:40,  1.56it/s]\u001b[A\n",
            "Iteration:   4% 42/947 [00:26<09:39,  1.56it/s]\u001b[A\n",
            "Iteration:   5% 43/947 [00:27<09:39,  1.56it/s]\u001b[A\n",
            "Iteration:   5% 44/947 [00:28<09:38,  1.56it/s]\u001b[A\n",
            "Iteration:   5% 45/947 [00:28<09:37,  1.56it/s]\u001b[A\n",
            "Iteration:   5% 46/947 [00:29<09:38,  1.56it/s]\u001b[A\n",
            "Iteration:   5% 47/947 [00:30<09:35,  1.56it/s]\u001b[A\n",
            "Iteration:   5% 48/947 [00:30<09:35,  1.56it/s]\u001b[A\n",
            "Iteration:   5% 49/947 [00:31<09:35,  1.56it/s]\u001b[A\n",
            "Iteration:   5% 50/947 [00:32<09:34,  1.56it/s]\u001b[A\n",
            "Iteration:   5% 51/947 [00:32<09:34,  1.56it/s]\u001b[A\n",
            "Iteration:   5% 52/947 [00:33<09:32,  1.56it/s]\u001b[A12/19/2020 23:26:36 - INFO - transformers.configuration_utils -   Configuration saved in essays_model/checkpoint-1000/config.json\n",
            "12/19/2020 23:26:39 - INFO - transformers.modeling_utils -   Model weights saved in essays_model/checkpoint-1000/pytorch_model.bin\n",
            "12/19/2020 23:26:39 - INFO - __main__ -   Saving model checkpoint to essays_model/checkpoint-1000\n",
            "12/19/2020 23:26:47 - INFO - __main__ -   Saving optimizer and scheduler states to essays_model/checkpoint-1000\n",
            "\n",
            "Iteration:   6% 53/947 [00:44<56:46,  3.81s/it]\u001b[A\n",
            "Iteration:   6% 54/947 [00:45<42:34,  2.86s/it]\u001b[A\n",
            "Iteration:   6% 55/947 [00:45<32:34,  2.19s/it]\u001b[A\n",
            "Iteration:   6% 56/947 [00:46<25:38,  1.73s/it]\u001b[A\n",
            "Iteration:   6% 57/947 [00:47<20:45,  1.40s/it]\u001b[A\n",
            "Iteration:   6% 58/947 [00:47<17:20,  1.17s/it]\u001b[A\n",
            "Iteration:   6% 59/947 [00:48<14:56,  1.01s/it]\u001b[A\n",
            "Iteration:   6% 60/947 [00:48<13:15,  1.11it/s]\u001b[A\n",
            "Iteration:   6% 61/947 [00:49<12:06,  1.22it/s]\u001b[A\n",
            "Iteration:   7% 62/947 [00:50<11:16,  1.31it/s]\u001b[A\n",
            "Iteration:   7% 63/947 [00:50<10:42,  1.38it/s]\u001b[A\n",
            "Iteration:   7% 64/947 [00:51<10:18,  1.43it/s]\u001b[A\n",
            "Iteration:   7% 65/947 [00:52<10:01,  1.47it/s]\u001b[A\n",
            "Iteration:   7% 66/947 [00:52<09:50,  1.49it/s]\u001b[A\n",
            "Iteration:   7% 67/947 [00:53<09:41,  1.51it/s]\u001b[A\n",
            "Iteration:   7% 68/947 [00:54<09:35,  1.53it/s]\u001b[A\n",
            "Iteration:   7% 69/947 [00:54<09:30,  1.54it/s]\u001b[A\n",
            "Iteration:   7% 70/947 [00:55<09:28,  1.54it/s]\u001b[A\n",
            "Iteration:   7% 71/947 [00:56<09:24,  1.55it/s]\u001b[A\n",
            "Iteration:   8% 72/947 [00:56<09:23,  1.55it/s]\u001b[A\n",
            "Iteration:   8% 73/947 [00:57<09:21,  1.56it/s]\u001b[A\n",
            "Iteration:   8% 74/947 [00:57<09:20,  1.56it/s]\u001b[A\n",
            "Iteration:   8% 75/947 [00:58<09:20,  1.56it/s]\u001b[A\n",
            "Iteration:   8% 76/947 [00:59<09:19,  1.56it/s]\u001b[A\n",
            "Iteration:   8% 77/947 [00:59<09:18,  1.56it/s]\u001b[A\n",
            "Iteration:   8% 78/947 [01:00<09:18,  1.56it/s]\u001b[A\n",
            "Iteration:   8% 79/947 [01:01<09:18,  1.55it/s]\u001b[A\n",
            "Iteration:   8% 80/947 [01:01<09:18,  1.55it/s]\u001b[A\n",
            "Iteration:   9% 81/947 [01:02<09:19,  1.55it/s]\u001b[A\n",
            "Iteration:   9% 82/947 [01:03<09:18,  1.55it/s]\u001b[A\n",
            "Iteration:   9% 83/947 [01:03<09:18,  1.55it/s]\u001b[A\n",
            "Iteration:   9% 84/947 [01:04<09:18,  1.54it/s]\u001b[A\n",
            "Iteration:   9% 85/947 [01:05<09:18,  1.54it/s]\u001b[A\n",
            "Iteration:   9% 86/947 [01:05<09:18,  1.54it/s]\u001b[A\n",
            "Iteration:   9% 87/947 [01:06<09:17,  1.54it/s]\u001b[A\n",
            "Iteration:   9% 88/947 [01:06<09:16,  1.54it/s]\u001b[A\n",
            "Iteration:   9% 89/947 [01:07<09:16,  1.54it/s]\u001b[A\n",
            "Iteration:  10% 90/947 [01:08<09:17,  1.54it/s]\u001b[A\n",
            "Iteration:  10% 91/947 [01:08<09:14,  1.54it/s]\u001b[A\n",
            "Iteration:  10% 92/947 [01:09<09:13,  1.55it/s]\u001b[A\n",
            "Iteration:  10% 93/947 [01:10<09:13,  1.54it/s]\u001b[A\n",
            "Iteration:  10% 94/947 [01:10<09:12,  1.54it/s]\u001b[A\n",
            "Iteration:  10% 95/947 [01:11<09:12,  1.54it/s]\u001b[A\n",
            "Iteration:  10% 96/947 [01:12<09:11,  1.54it/s]\u001b[A\n",
            "Iteration:  10% 97/947 [01:12<09:10,  1.54it/s]\u001b[A\n",
            "Iteration:  10% 98/947 [01:13<09:10,  1.54it/s]\u001b[A\n",
            "Iteration:  10% 99/947 [01:14<09:09,  1.54it/s]\u001b[A\n",
            "Iteration:  11% 100/947 [01:14<09:08,  1.54it/s]\u001b[A\n",
            "Iteration:  11% 101/947 [01:15<09:08,  1.54it/s]\u001b[A\n",
            "Iteration:  11% 102/947 [01:16<09:07,  1.54it/s]\u001b[A\n",
            "Iteration:  11% 103/947 [01:16<09:06,  1.54it/s]\u001b[A\n",
            "Iteration:  11% 104/947 [01:17<09:06,  1.54it/s]\u001b[A\n",
            "Iteration:  11% 105/947 [01:18<09:05,  1.54it/s]\u001b[A\n",
            "Iteration:  11% 106/947 [01:18<09:04,  1.54it/s]\u001b[A\n",
            "Iteration:  11% 107/947 [01:19<09:04,  1.54it/s]\u001b[A\n",
            "Iteration:  11% 108/947 [01:19<09:03,  1.54it/s]\u001b[A\n",
            "Iteration:  12% 109/947 [01:20<09:02,  1.54it/s]\u001b[A\n",
            "Iteration:  12% 110/947 [01:21<09:02,  1.54it/s]\u001b[A\n",
            "Iteration:  12% 111/947 [01:21<09:02,  1.54it/s]\u001b[A\n",
            "Iteration:  12% 112/947 [01:22<09:00,  1.54it/s]\u001b[A\n",
            "Iteration:  12% 113/947 [01:23<08:59,  1.55it/s]\u001b[A\n",
            "Iteration:  12% 114/947 [01:23<08:58,  1.55it/s]\u001b[A\n",
            "Iteration:  12% 115/947 [01:24<08:58,  1.55it/s]\u001b[A\n",
            "Iteration:  12% 116/947 [01:25<08:56,  1.55it/s]\u001b[A\n",
            "Iteration:  12% 117/947 [01:25<08:55,  1.55it/s]\u001b[A\n",
            "Iteration:  12% 118/947 [01:26<08:53,  1.55it/s]\u001b[A\n",
            "Iteration:  13% 119/947 [01:27<08:53,  1.55it/s]\u001b[A\n",
            "Iteration:  13% 120/947 [01:27<08:52,  1.55it/s]\u001b[A\n",
            "Iteration:  13% 121/947 [01:28<08:52,  1.55it/s]\u001b[A\n",
            "Iteration:  13% 122/947 [01:28<08:52,  1.55it/s]\u001b[A\n",
            "Iteration:  13% 123/947 [01:29<08:49,  1.55it/s]\u001b[A\n",
            "Iteration:  13% 124/947 [01:30<08:48,  1.56it/s]\u001b[A\n",
            "Iteration:  13% 125/947 [01:30<08:48,  1.56it/s]\u001b[A\n",
            "Iteration:  13% 126/947 [01:31<08:46,  1.56it/s]\u001b[A\n",
            "Iteration:  13% 127/947 [01:32<08:46,  1.56it/s]\u001b[A\n",
            "Iteration:  14% 128/947 [01:32<08:46,  1.56it/s]\u001b[A\n",
            "Iteration:  14% 129/947 [01:33<08:45,  1.56it/s]\u001b[A\n",
            "Iteration:  14% 130/947 [01:34<08:44,  1.56it/s]\u001b[A\n",
            "Iteration:  14% 131/947 [01:34<08:43,  1.56it/s]\u001b[A\n",
            "Iteration:  14% 132/947 [01:35<08:43,  1.56it/s]\u001b[A\n",
            "Iteration:  14% 133/947 [01:36<08:42,  1.56it/s]\u001b[A\n",
            "Iteration:  14% 134/947 [01:36<08:41,  1.56it/s]\u001b[A\n",
            "Iteration:  14% 135/947 [01:37<08:40,  1.56it/s]\u001b[A\n",
            "Iteration:  14% 136/947 [01:37<08:40,  1.56it/s]\u001b[A\n",
            "Iteration:  14% 137/947 [01:38<08:38,  1.56it/s]\u001b[A\n",
            "Iteration:  15% 138/947 [01:39<08:38,  1.56it/s]\u001b[A\n",
            "Iteration:  15% 139/947 [01:39<08:37,  1.56it/s]\u001b[A\n",
            "Iteration:  15% 140/947 [01:40<08:36,  1.56it/s]\u001b[A\n",
            "Iteration:  15% 141/947 [01:41<08:37,  1.56it/s]\u001b[A\n",
            "Iteration:  15% 142/947 [01:41<08:36,  1.56it/s]\u001b[A\n",
            "Iteration:  15% 143/947 [01:42<08:36,  1.56it/s]\u001b[A\n",
            "Iteration:  15% 144/947 [01:43<08:34,  1.56it/s]\u001b[A\n",
            "Iteration:  15% 145/947 [01:43<08:33,  1.56it/s]\u001b[A\n",
            "Iteration:  15% 146/947 [01:44<08:32,  1.56it/s]\u001b[A\n",
            "Iteration:  16% 147/947 [01:45<08:31,  1.56it/s]\u001b[A\n",
            "Iteration:  16% 148/947 [01:45<08:30,  1.57it/s]\u001b[A\n",
            "Iteration:  16% 149/947 [01:46<08:29,  1.57it/s]\u001b[A\n",
            "Iteration:  16% 150/947 [01:46<08:28,  1.57it/s]\u001b[A\n",
            "Iteration:  16% 151/947 [01:47<08:27,  1.57it/s]\u001b[A\n",
            "Iteration:  16% 152/947 [01:48<08:27,  1.57it/s]\u001b[A\n",
            "Iteration:  16% 153/947 [01:48<08:25,  1.57it/s]\u001b[A\n",
            "Iteration:  16% 154/947 [01:49<08:25,  1.57it/s]\u001b[A\n",
            "Iteration:  16% 155/947 [01:50<08:24,  1.57it/s]\u001b[A\n",
            "Iteration:  16% 156/947 [01:50<08:24,  1.57it/s]\u001b[A\n",
            "Iteration:  17% 157/947 [01:51<08:22,  1.57it/s]\u001b[A\n",
            "Iteration:  17% 158/947 [01:52<08:23,  1.57it/s]\u001b[A\n",
            "Iteration:  17% 159/947 [01:52<08:22,  1.57it/s]\u001b[A\n",
            "Iteration:  17% 160/947 [01:53<08:24,  1.56it/s]\u001b[A\n",
            "Iteration:  17% 161/947 [01:53<08:23,  1.56it/s]\u001b[A\n",
            "Iteration:  17% 162/947 [01:54<08:22,  1.56it/s]\u001b[A\n",
            "Iteration:  17% 163/947 [01:55<08:21,  1.56it/s]\u001b[A\n",
            "Iteration:  17% 164/947 [01:55<08:20,  1.56it/s]\u001b[A\n",
            "Iteration:  17% 165/947 [01:56<08:19,  1.56it/s]\u001b[A\n",
            "Iteration:  18% 166/947 [01:57<08:18,  1.57it/s]\u001b[A\n",
            "Iteration:  18% 167/947 [01:57<08:18,  1.57it/s]\u001b[A\n",
            "Iteration:  18% 168/947 [01:58<08:17,  1.57it/s]\u001b[A\n",
            "Iteration:  18% 169/947 [01:59<08:17,  1.56it/s]\u001b[A\n",
            "Iteration:  18% 170/947 [01:59<08:16,  1.57it/s]\u001b[A\n",
            "Iteration:  18% 171/947 [02:00<08:15,  1.57it/s]\u001b[A\n",
            "Iteration:  18% 172/947 [02:00<08:15,  1.56it/s]\u001b[A\n",
            "Iteration:  18% 173/947 [02:01<08:13,  1.57it/s]\u001b[A\n",
            "Iteration:  18% 174/947 [02:02<08:13,  1.57it/s]\u001b[A\n",
            "Iteration:  18% 175/947 [02:02<08:12,  1.57it/s]\u001b[A\n",
            "Iteration:  19% 176/947 [02:03<08:12,  1.57it/s]\u001b[A\n",
            "Iteration:  19% 177/947 [02:04<08:10,  1.57it/s]\u001b[A\n",
            "Iteration:  19% 178/947 [02:04<08:10,  1.57it/s]\u001b[A\n",
            "Iteration:  19% 179/947 [02:05<08:11,  1.56it/s]\u001b[A\n",
            "Iteration:  19% 180/947 [02:06<08:08,  1.57it/s]\u001b[A\n",
            "Iteration:  19% 181/947 [02:06<08:08,  1.57it/s]\u001b[A\n",
            "Iteration:  19% 182/947 [02:07<08:08,  1.57it/s]\u001b[A\n",
            "Iteration:  19% 183/947 [02:07<08:07,  1.57it/s]\u001b[A\n",
            "Iteration:  19% 184/947 [02:08<08:06,  1.57it/s]\u001b[A\n",
            "Iteration:  20% 185/947 [02:09<08:07,  1.56it/s]\u001b[A"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2N6ylGPt1F5"
      },
      "source": [
        "## Check our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XM10Ca8uEVN"
      },
      "source": [
        "## Select topic\n",
        "for idx in range(len(valid)):\n",
        "  if \"образует его общество\" in valid[idx]:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKQ_Isld3xZ3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "f13e85bf-aaa8-4612-fa52-3e2a52f5402a"
      },
      "source": [
        "valid[idx]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<s>Тема: «Создает человека природа, но развивает и образует его общество». (В.Т. Белинский)\\nСочинение: Эссе №1Человек это высшая ступень развития живых организмов на земле, субъект общественно-исторической деятельности и культуры, но важнейшей его характеристикой является биосоциальная сущность.Белинский В.Г. в своем выражении очень точно и ёмко охарактеризован двойственную природу человека. Во-первых, человек есть порождение природы, является цепью эволюции, таким же организмом, как и всё, что нас окружает. По биологическим признакам человек ничем не отличается от животного. Во-вторых, он есть порождение общества. Этот момент более сложен. Ясно только то, что благодаря общественному развитию человек стал человеком.Человек без общества ничто, недаром в древности изгнание из общества являлось самим страшным наказанием. Очень много в современном мире примеров синдрома«маугли», когда воспитание ребенок получил от животного и поэтому ведет себя как он, а не как человек, что говорит лишь о том, что социальное в человеке не заложено генетически, а дается обществом. Так же подтверждает это и роман Даниэля Дефо«Робинзон Крузо». Без знаний, которые накопило общество, Робинзону Крузо было бытяжело выжить. А может и невозможно. Он старался повторить, познать всё то, что было создано в обществе.Подводя итог, хотелось бы отметить, что человек и общество понятия неразделимые. Именно благодаря обществу человек как существо .биологическое смог стать человеком, таким, какой он есть. Тело человеку дает природа, а разум и душу - общество.Эссе №2Каждый человек в широком смысле это «дитя природы». Согласно биологическим, закономерностям, человек обособился и развился из животного мира. Поэтому животные инстинкты вполне объяснимы в человеческой сущности, они имеют естественное происхождение. Однако человек ничем бы не отличался от животного, если бы эти инстинкты, дарованные природой, составляли бы его глубинную первооснову и определяли все его существование.Определяющее воздействие на складывание человека оказывает общество. Под обществом в данном случае мы понимаем обособившуюся от природы (совокупности естественных условий существования человека) часть мира. Утвердившиеся моральные нормы и правила поведения, культурные достижения, политико-правовые особенности, социально-экономические отношения - всё это разнообразные составляющие общества в целом.Только в обществе человек приобретает личностные характеристики (то есть такие социально значимые черты, которые характеризуют индивида как члена того или иного общества). Таким образом, на мой взгляд, В.Г. Белинский глубоко был прав, отмечая, что биологически человека создаёт природа; но сущностные характеристики человеческая личность приобретает и развивает в обществе, во взаимодействии с другими личностями, вступая с ними в разнообразные отношения.С другой стороны, представляется, что в данном высказывании В.Г. Белинского два этих понятия - «общество» и «природа» - выступают как диаметральные противоположности. Мне не представляется это правильным. Человек, общество и природа очень тесно взаимосвязаны, влияют друг на друга. Известно, что, с одной стороны, природная среда, географические и климатические особенности оказывают значительное воздействие на общественное развитие, ускоряя или замедляя его темп и, в конечном счете, определяют ментальность народа (как совокупность общественных ценностей, установок, готовности действовать или мыслить определенным образом). С другой стороны, и общество влияет на естественную среду обитания человека. В последнее время чаще всего отмечают негативное воздействие человеческого общества на экологическую обстановку.Таким образом, завершая наш небольшой анализ, отметим, что природа и общество являются двумя основными составляющими, тесно взаимосвязанными, взаимодействующими, которые определяют особенности складывания и формирования человека как личности. Причем второй компонент (общество) в настоящее время оказывает непосредственное и наиболее сильное влияние; а воздействие природы в современном мире во многом опосредовано.</s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRlAAsIbsHdf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04ce6f1f-641f-4704-c477-103d4e098d54"
      },
      "source": [
        "!python generate_transformers.py \\\n",
        "    --model_type=gpt2 \\\n",
        "    --model_name_or_path=essays_model \\\n",
        "    --k=5 \\\n",
        "    --p=0.95 \\\n",
        "    --length=500"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-28 15:45:33.604619: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "10/28/2020 15:45:35 - INFO - transformers.tokenization_utils -   Model name 'essays_model' not found in model shortcut name list (gpt2, gpt2-medium, gpt2-large, gpt2-xl, distilgpt2). Assuming 'essays_model' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
            "10/28/2020 15:45:35 - INFO - transformers.tokenization_utils -   Didn't find file essays_model/added_tokens.json. We won't load it.\n",
            "10/28/2020 15:45:35 - INFO - transformers.tokenization_utils -   loading file essays_model/vocab.json\n",
            "10/28/2020 15:45:35 - INFO - transformers.tokenization_utils -   loading file essays_model/merges.txt\n",
            "10/28/2020 15:45:35 - INFO - transformers.tokenization_utils -   loading file None\n",
            "10/28/2020 15:45:35 - INFO - transformers.tokenization_utils -   loading file essays_model/special_tokens_map.json\n",
            "10/28/2020 15:45:35 - INFO - transformers.tokenization_utils -   loading file essays_model/tokenizer_config.json\n",
            "10/28/2020 15:45:35 - INFO - transformers.configuration_utils -   loading configuration file essays_model/config.json\n",
            "10/28/2020 15:45:35 - INFO - transformers.configuration_utils -   Model config GPT2Config {\n",
            "  \"_num_labels\": 2,\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bad_words_ids\": null,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"decoder_start_token_id\": null,\n",
            "  \"do_sample\": false,\n",
            "  \"early_stopping\": false,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\",\n",
            "    \"1\": \"LABEL_1\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"is_decoder\": false,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0,\n",
            "    \"LABEL_1\": 1\n",
            "  },\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"length_penalty\": 1.0,\n",
            "  \"max_length\": 20,\n",
            "  \"min_length\": 0,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 2048,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 2048,\n",
            "  \"no_repeat_ngram_size\": 0,\n",
            "  \"num_beams\": 1,\n",
            "  \"num_return_sequences\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": null,\n",
            "  \"prefix\": null,\n",
            "  \"pruned_heads\": {},\n",
            "  \"repetition_penalty\": 1.0,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": null,\n",
            "  \"temperature\": 1.0,\n",
            "  \"top_k\": 50,\n",
            "  \"top_p\": 1.0,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "10/28/2020 15:45:35 - INFO - transformers.modeling_utils -   loading weights file essays_model/pytorch_model.bin\n",
            "10/28/2020 15:45:46 - INFO - __main__ -   Namespace(device=device(type='cuda'), k=5, length=500, model_name_or_path='essays_model', model_type='gpt2', n_gpu=1, no_cuda=False, num_return_sequences=1, p=0.95, padding_text='', prompt='', repetition_penalty=1.0, seed=42, stop_token='</s>', temperature=1.0, xlm_language='')\n",
            "Context >>> <s>Тема: «Создает человека природа, но развивает и образует его общество». (В.Т. Белинский)\\nСочинение:\n",
            "10/28/2020 15:45:51 - WARNING - transformers.modeling_utils -   Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n",
            "ruGPT:\n",
            "<s>Тема: «Создает человека природа, но развивает и образует его общество». (В.Т. Белинский)\\nСочинение: В основе существования человека лежит биосоциальная сущность. Человек, будучи существом разумным, должен был бы приспособиться к окружающей его среде, жить в обществе, удовлетворять свои физиологические потребности, быть активным членом общества. Но природа не приспособлена к человеку. Она неорганизованная система, она не может существовать без общества. Поэтому я считаю, что В.Т. Белинский хотел указать на два фактора, сыгравших определяющую роль в формировании человека: на биологическое родство и на социальное родство. Человек - это высшая форма организации живых организмов. Он является органической совокупностью, состоящей из высших органических и неорганических элементов, а также нервной ткани, клеточных мембран, а также органов и систем, которые в свою очередь состоят из простых органических элементов. Организмы существуют независимо друг от друга. Одни - это биосоциальные, другие - социальные. Одни - это сложные организмы, которые состоят из простых органических элементов. Одни организмы существуют обособленно, другие - сообщат друг другу свои преимущества. В естественных условиях человеку необходимо общение, т.е. взаимодействие живых организмов друг с другом. Организмы могут существовать в условиях изоляции, изоляции от других организмов, поэтому я считаю, что В.Т. Белинский хотел указать, что общение является одним из способов сохранения жизни. В естественных условиях человек может жить только в обществе, в котором существует разделение властей и которое защищает его права и интересы. Общество - это система общественных отношений, основанная на взаимной помощи, взаимном признании, взаимной ответственности, взаимодействии между людьми. В естественных условиях человек ничем не отличается от животного, но он обладает рядом важных социальных свойств. Животные обладают развитым интеллектом, способностью обучаться, общаться с другими животными. Животные не подчиняются законам природы, они лишь выполняют функцию воспроизводства потомства, поэтому я считаю, что В.Т. Белинский хотел указать на два этих фактора, сыгравших определяющую роль в формировании человека: на биологическое родство и на социальное родство. Человек, по моему мнению, является высшей формой организации живых организмов. Он является органической совокупностью, состоящей из высших органических и неорганических элементов, и только благодаря общению с другими животными человек может существовать в обществе.\n",
            "Context >>> ^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kpMtmoxvQ3a"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}